#!/usr/bin/env python3.8
#####################################################
##              Align Depth to Color               ##
#####################################################
import pyrealsense2 as rs
import numpy as np
import cv2
import open3d as o3d
from ultralytics import YOLO
import os
import rospy
from tm_msgs.msg import FeedbackState
from tm_msgs.srv import AskItem, AskItemRequest
from scipy.spatial.transform import Rotation as R_scipy
import re
from sensor_msgs.msg import PointCloud2, PointField
from std_msgs.msg import Header
from visualization_msgs.msg import Marker,MarkerArray
import sensor_msgs.point_cloud2 as pcl2
from visualization_msgs.msg import MarkerArray  # Â¶ÇÊûúÊú™‰æÜÊîπÁî® MarkerArray
from std_msgs.msg import ColorRGBA    
import math
import sys
sys.path.append('/home/chen/catkin_ws/src')  # Â∞á src Âä†ÂÖ•Ê®°ÁµÑË∑ØÂæë
from robot_control import *
from collections import defaultdict

# from node_control.script.listen_python_move_to_fixture import new_monitor,move_send_script
from gpd_ros.msg import GraspConfigList

class_names_dict={0: 'aluextru', 1: 'bin', 2: 'twpipe',3:'unknown'}  
class_colors = {
    0: (255, 0, 0),   # È°ûÂà• 0 - Á¥ÖËâ≤
    1: (0, 255, 0),   # È°ûÂà• 1 - Á∂†Ëâ≤
    2: (0, 0, 255),   # È°ûÂà• 2 - ËóçËâ≤
    3:(127.5,127.5,127.5), # È°ûÂà• 3 - ÁÅ∞Ëâ≤
    # Ê∑ªÂä†Êõ¥Â§öÈ°ûÂà•È°èËâ≤
}

image_path_test='/home/chen/catkin_ws/src/pcl_with_gpd/picture'
# weight_path = r"/home/chen/catkin_ws/src/pcl_with_gpd/weight/New_best.pt"
weight_path = r"/home/chen/catkin_ws/src/pcl_with_gpd/weight/2025_06_28_best.pt"
model = YOLO(weight_path)

# exit()
# print(dir(rs))
# help(rs.rs2_deproject_pixel_to_point)
class view_point_cloud:
    def __init__(self,view_id,color_image=None,depth_image=None,intrinsics=None):
        self.intrinsics=intrinsics
        self.color_image=color_image
        self.depth_image=depth_image
        self.mark_image,self.mask_pixel_list,self.class_id_list,self.confidence_score_list=self.get_mask_data_accurate(color_image)
        
        self.all_instance_points,self.all_instance_original_colors,self.all_instance_draw_colors=self.merge_mask_points_v2()
    
        self.o3d_pcd=self.generate_o3d_pointcloud()
        
        self.view_id=view_id

    
    def extract_pose_values(self,coord_string):
        # ‰ΩøÁî®Ê≠£ÂâáË°®ÈÅîÂºèÊèêÂèñËä±Êã¨ËôüÂÖßÁöÑÂÖßÂÆπ
        match = re.search(r'\{(.*?)\}', coord_string)
        if match:
            # Â∞áÊèêÂèñÁöÑÂÖßÂÆπÊåâÈÄóËôüÂàÜÂâ≤
            values_str = match.group(1).split(',')
            # Â∞áÂ≠óÁ¨¶‰∏≤ËΩâÊèõÁÇ∫ÊµÆÈªûÊï∏
            values = [float(v.strip()) for v in values_str]
            return values
        else:
            return None


    def ask_flange_pose(self):
        # global robot_pose
        client = rospy.ServiceProxy('tm_driver/ask_item', AskItem)
        request = AskItemRequest()
        request.id = "demo"
        request.wait_time = 0.5
        request.item = "Coord_Robot_Flange"
        try:
            response = client(request)
            if response.ok:
                rospy.loginfo("AskItem to robot: item is {}, value is {}\n".format(request.item, response.value))
                coord_string = response.value
                pose_values = self.extract_pose_values(coord_string)
                if pose_values:
                    pose_values = np.asarray(pose_values)
                    # robot_pose=pose_values
                    return pose_values
        except rospy.ServiceException as e:
                    rospy.logerr("Error AskItem to robot: {}".format(e))


    def pose_to_matrix(self,pose, degrees=True,milimeter=True):
        """
        Â∞á‰ΩçÁΩÆ (x, y, z) ÂíåÊ≠êÊãâËßí (rx, ry, rz) ËΩâÊèõÁÇ∫ 4x4 ÁöÑÈΩäÊ¨°ËΩâÊèõÁü©Èô£„ÄÇ
        
        ÂèÉÊï∏:
            x, y, z: Âπ≥ÁßªÈáè
            rx, ry, rz: Ê≠êÊãâËßíÔºàÂñÆ‰ΩçÁÇ∫ÂºßÂ∫¶ÊàñÂ∫¶Ôºâ
            degrees: Â¶ÇÊûúÁÇ∫ TrueÔºåÂâáËº∏ÂÖ•ÁöÑËßíÂ∫¶ÁÇ∫Â∫¶ÔºõÂê¶ÂâáÁÇ∫ÂºßÂ∫¶„ÄÇ
        
        ËøîÂõû:
            4x4 ÁöÑÈΩäÊ¨°ËΩâÊèõÁü©Èô£Ôºànumpy.ndarrayÔºâ
            
        """
        x,y,z,rx,ry,rz=pose
        if degrees:
            rotation = R_scipy.from_euler('xyz', [rx, ry, rz], degrees=True)
        else:
            rotation = R_scipy.from_euler('xyz', [rx, ry, rz], degrees=False)
        matrix = np.eye(4)
        
        matrix[:3, :3] = rotation.as_matrix() 
        
        if milimeter:
            
            matrix[:3, 3] = [x/1000, y/1000, z/1000]
        else:
            matrix[:3, 3] = [x, y, z]
        return matrix


    def matrix_to_pose(self,matrix, degrees=True,milimeter=True):
        """
        Â∞á 4x4 ÁöÑÈΩäÊ¨°ËΩâÊèõÁü©Èô£ÈÇÑÂéüÁÇ∫‰ΩçÁΩÆÂíåÊ≠êÊãâËßí„ÄÇ
        
        ÂèÉÊï∏:
            matrix: 4x4 ÁöÑÈΩäÊ¨°ËΩâÊèõÁü©Èô£Ôºànumpy.ndarrayÔºâ
            degrees: Â¶ÇÊûúÁÇ∫ TrueÔºåÂâáËº∏Âá∫ÁöÑËßíÂ∫¶ÁÇ∫Â∫¶ÔºõÂê¶ÂâáÁÇ∫ÂºßÂ∫¶„ÄÇ
        
        ËøîÂõû:
            (x, y, z, rx, ry, rz): ‰ΩçÁΩÆÂíåÂπ≥ÁßªÈáè
        """
        if degrees:
            rotation = R_scipy.from_matrix(matrix[:3, :3])
            rx, ry, rz = rotation.as_euler('xyz', degrees=True)
        else:
            rotation = R_scipy.from_matrix(matrix[:3, :3])
            rx, ry, rz = rotation.as_euler('xyz', degrees=False)

        if milimeter:
            x, y, z = matrix[:3, 3]/1000
        else:
            x,y,z=matrix[:3, 3]
        return x, y, z, rx, ry, rz


    '''def get_mask_data(self,image_src):
        image=image_src.copy()
        results = model.predict(source=image,verbose=False)
        
        # ËÆÄÂèñÂéüÂßãÂúñÁâá
        # original_image = cv2.imread(image)
        original_image = image
        mask_pixel_list=[]
        class_id_list=[]
        confidence_score_list=[]

        for result in results:
            masks = result.masks  # Áç≤ÂèñÂàÜÂâ≤ÈÅÆÁΩ©  
            classes = result.boxes.cls.cpu().numpy()  # Áç≤ÂèñÈ°ûÂà•Á¥¢Âºï  
            confidence=result.boxes.conf.cpu().numpy()  # Áç≤ÂèñÁΩÆ‰ø°Â∫¶
            if masks is not None:
                for mask, cls,conf in zip(masks.data, classes,confidence):
                    # Â∞áÈÅÆÁΩ©ËΩâÊèõÁÇ∫‰∫åÂÄºÂúñÂÉè
                    mask = mask.cpu().numpy().astype(np.uint8) * 255
                    # print(conf)
                    confidence_score_list.append(conf)
                    
                    # Áç≤ÂèñÂ∞çÊáâÈ°ûÂà•ÁöÑÈ°èËâ≤
                    color = class_colors.get(int(cls), (255, 255, 255))  # ÈªòË™çÁôΩËâ≤
                    class_name = class_names_dict.get(int(cls), "Unknown")
                    class_id_list.append(class_name)

                    # Áç≤ÂèñÈÅÆÁΩ©ÂÖßÈÉ®ÊâÄÊúâÂÉèÁ¥†ÁöÑÂ∫ßÊ®ô
                    y_coords, x_coords = np.where(mask > 0)  # Áç≤ÂèñÈùûÈõ∂ÂÉèÁ¥†ÁöÑ y Âíå x Â∫ßÊ®ô
                    pixel_coords = np.array([x_coords, y_coords])  # 2√óN Áü©Èô£
                    mask_pixel_list.append(pixel_coords.T)

                    """ 
                    # Ê∏¨Ë©¶ÈÅÆÁΩ©Â∫ßÊ®ôÊòØÂê¶Ê≠£Á¢∫
                    for x,y in pixel_coords.T:
                        original_image[y,x]=(0,0,0)
                    print(pixel_coords[:,0])
                    """

                    # exit()

                    # Â∞áÈÅÆÁΩ©ÁñäÂä†Âà∞ÂéüÂßãÂúñÁâá
                    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                    cv2.drawContours(original_image, contours, -1, color, thickness=1)
                    
                    # ÂâµÂª∫‰∏ÄÂÄãËàáÂéüÂßãÂúñÁâáÁõ∏ÂêåÂ§ßÂ∞èÁöÑÈÄèÊòéÈÅÆÁΩ©
                    overlay = original_image.copy()
                    alpha = 0.5  # Ë®≠ÂÆöÈÄèÊòéÂ∫¶ (0.0 ÂÆåÂÖ®ÈÄèÊòé, 1.0 ÂÆåÂÖ®‰∏çÈÄèÊòé)

                    # Â∞áÈÅÆÁΩ©ÊáâÁî®È°èËâ≤
                    overlay[mask > 0] = color

                    # Â∞áÈÄèÊòéÈÅÆÁΩ©ÁñäÂä†Âà∞ÂéüÂßãÂúñÁâá
                    cv2.addWeighted(overlay, alpha, original_image, 1 - alpha, 0, original_image)
                        # Ë®àÁÆóÈÅÆÁΩ©ÁöÑ‰∏≠ÂøÉÈªû
                    for contour in contours:
                        M = cv2.moments(contour)
                        if M["m00"] > 0:  # Èò≤Ê≠¢Èô§‰ª•Èõ∂
                            cX = int(M["m10"] / M["m00"])
                            cY = int(M["m01"] / M["m00"])
                            # Âú®‰∏≠ÂøÉÈªûÊ∑ªÂä†È°ûÂà•ÊñáÂ≠óÊ®ôÁ±§
                            cv2.putText(original_image, class_name, (cX, cY), cv2.FONT_HERSHEY_SIMPLEX, 
                                        0.6, color, 2, cv2.LINE_AA)
                    print(class_name,conf,pixel_coords.T.shape)
        self.mark_image=original_image
        self.mask_pixel_list=mask_pixel_list
        self.class_id_list=class_id_list
        self.confidence_score_list=confidence_score_list
                    # break
        return original_image,mask_pixel_list,class_id_list,confidence_score_list'''


    def pixel_to_3d_points(self,mask_pixel_np, depth_image, intrinsics ):
        """
        Â∞á mask ÁöÑ N x 2 pixel Â∫ßÊ®ôËΩâÁÇ∫ 3D Á©∫ÈñìÂ∫ßÊ®ôÔºàN x 3Ôºâ

        mask_pixel_np: (N, 2)ÔºåÊØèÂàóÊòØ [x, y]
        depth_image: Â∞çÈΩä color ÁöÑÊ∑±Â∫¶ÂúñÔºàÂñÆ‰Ωç mmÔºâ
        intrinsics: RealSense Áõ∏Ê©üÂÖßÂèÉÔºàÂ∞çÈΩä color Áî® color stream ÁöÑÂÖßÂèÉÔºâ
        """
        points_3d = []
        pose = self.ask_flange_pose()
        T_flange2base = self.pose_to_matrix(pose)
        T_cam2gripper=np.array(
        [[-0.995,0.099,-0.017,0.038],
        [-0.098,-0.992,-0.079,-0.075],
        [-0.025,-0.076,0.997,0.038],
        [0.000,0.000,0.000,1.000]])

        for x, y in mask_pixel_np:
            depth = depth_image[y, x] * 0.001  # mm ‚Üí m
            if depth == 0: continue  # ÂøΩÁï•ÁÑ°ÊïàÈªû
            pt3d = rs.rs2_deproject_pixel_to_point(intrinsics, [x, y], depth)  # ÂõûÂÇ≥ [X, Y, Z]
            points_3d.append(pt3d)
        
        
        # Â∞á points_3d ËΩâÊèõÁÇ∫ 4 x M ÁöÑÈΩäÊ¨°Áü©Èô£ÂΩ¢Âºè
        points_3d = np.array(points_3d)  # ÂéüÂßã M x 3
        points_3d_homogeneous = np.hstack((points_3d, np.ones((points_3d.shape[0], 1)))).T  # ËΩâÊèõÁÇ∫ 4 x M
        points_3d_in_base= (T_flange2base @ T_cam2gripper @ points_3d_homogeneous).T[:, :3]  # ËΩâÊèõÁÇ∫ base Â∫ßÊ®ôÁ≥ª
        return points_3d_in_base # shape (M, 3)


    def merge_mask_points(self):
        """
        Â∞á mask ÁöÑ N x 2 pixel Â∫ßÊ®ôËΩâÁÇ∫ 3D Á©∫ÈñìÂ∫ßÊ®ôÔºàN x 3Ôºâ

        mask_pixel_np: (N, 2)ÔºåÊØèÂàóÊòØ [x, y]
        depth_image: Â∞çÈΩä color ÁöÑÊ∑±Â∫¶ÂúñÔºàÂñÆ‰Ωç mmÔºâ
        intrinsics: RealSense Áõ∏Ê©üÂÖßÂèÉÔºàÂ∞çÈΩä color Áî® color stream ÁöÑÂÖßÂèÉÔºâ
        """
        all_instance_points = []
        all_instance_original_colors = []  # Êñ∞Â¢û‰∏ÄÂÄãÂàóË°®‰æÜÂ≠òÂÑ≤ÊØèÂÄãÂØ¶‰æãÁöÑÈ°èËâ≤
        all_instance_draw_colors = []  # Êñ∞Â¢û‰∏ÄÂÄãÂàóË°®‰æÜËëóËâ≤

        for i, pix2d in enumerate(self.mask_pixel_list):
            valid_pix2d = pix2d[self.depth_image[pix2d[:, 1], pix2d[:, 0]] > 0]  # ÈÅéÊøæÊéâÊ∑±Â∫¶ÂÄºÁÇ∫ 0 ÁöÑÂÉèÁ¥†
            # print(valid_pix2d.shape)
            pts3d = self.pixel_to_3d_points(valid_pix2d, self.depth_image, self.intrinsics)
            all_instance_points.append(pts3d)


            # Ê†πÊìöÈ°ûÂà•ÂêçÁ®±Êü•ÊâæÈ°èËâ≤
            class_name = self.class_id_list[i]
            class_index = next((k for k, v in class_names_dict.items() if v == class_name), None)
            color = class_colors.get(class_index, (255, 255, 255))  # ÈªòË™çÁôΩËâ≤
            normalized_color = [c / 255.0 for c in color]
            all_instance_draw_colors.extend([normalized_color] * len(pts3d))

            # ÊèêÂèñÊúâÊïàÂÉèÁ¥†ÁöÑÂéüÂßãÈ°èËâ≤
            original_colors = color_image[valid_pix2d[:, 1], valid_pix2d[:, 0]]
            normalized_colors = original_colors / 255.0
            all_instance_original_colors.extend(normalized_colors)
            print(np.array(all_instance_draw_colors).shape,np.array(all_instance_original_colors).shape)
        self.all_instance_points = all_instance_points
        self.all_instance_original_colors = all_instance_original_colors
        self.all_instance_draw_colors = all_instance_draw_colors
        return all_instance_points, all_instance_original_colors, all_instance_draw_colors
    
    def merge_mask_points_v1(self):
        all_instance_points = []
        all_instance_original_colors = []
        all_instance_draw_colors = []

        # ========== ËíêÈõÜÊâÄÊúâ mask ÁöÑÂÉèÁ¥† ==========
        all_mask_coords = set()

        for i, pix2d in enumerate(self.mask_pixel_list):
            for x, y in pix2d:
                all_mask_coords.add((x, y))

        # ========== Êï¥ÂºµÂΩ±ÂÉè‰∏≠ÁöÑÊúâÊïàÂÉèÁ¥† ==========
        h, w = self.depth_image.shape
        all_coords = [(x, y) for y in range(h) for x in range(w) if self.depth_image[y, x] > 0]

        # ========== ÊâæÂá∫ËÉåÊôØÂÉèÁ¥† ==========
        background_coords = [np.array([x, y]) for (x, y) in all_coords if (x, y) not in all_mask_coords]
        if background_coords:
            background_coords = np.array(background_coords)
            bg_pts3d = self.pixel_to_3d_points(background_coords, self.depth_image, self.intrinsics)
            all_instance_points.append(bg_pts3d)

            bg_color = [0.5, 0.5, 0.5]  # ËÉåÊôØÁµ¶ÁÅ∞Ëâ≤
            all_instance_draw_colors.extend([bg_color] * len(bg_pts3d))

            bg_original_colors = self.color_image[background_coords[:, 1], background_coords[:, 0]] / 255.0
            all_instance_original_colors.extend(bg_original_colors)

        # ========== ËôïÁêÜÊØèÂÄãÂØ¶‰æã ==========
        for i, pix2d in enumerate(self.mask_pixel_list):
            valid_pix2d = pix2d[self.depth_image[pix2d[:, 1], pix2d[:, 0]] > 0]
            pts3d = self.pixel_to_3d_points(valid_pix2d, self.depth_image, self.intrinsics)
            all_instance_points.append(pts3d)

            class_name = self.class_id_list[i]
            class_index = next((k for k, v in class_names_dict.items() if v == class_name), None)
            color = class_colors.get(class_index, (255, 255, 255))
            normalized_color = [c / 255.0 for c in color]
            all_instance_draw_colors.extend([normalized_color] * len(pts3d))

            original_colors = self.color_image[valid_pix2d[:, 1], valid_pix2d[:, 0]]
            normalized_colors = original_colors / 255.0
            all_instance_original_colors.extend(normalized_colors)

        self.all_instance_points = all_instance_points
        self.all_instance_original_colors = all_instance_original_colors
        self.all_instance_draw_colors = all_instance_draw_colors
        return all_instance_points, all_instance_original_colors, all_instance_draw_colors

    def merge_mask_points_v2(self):
        all_instance_points = []
        all_instance_original_colors = []
        all_instance_draw_colors = []

        # === ËíêÈõÜÊâÄÊúâÂâçÊôØÂØ¶‰æãÁöÑÂÉèÁ¥†Â∫ßÊ®ô ===
        foreground_coords_set = set()
        for pix2d in self.mask_pixel_list:
            for x, y in pix2d:
                foreground_coords_set.add((x, y))  # ËΩâÁÇ∫ tuple ‰ª•‰æøÂø´ÈÄüÊü•Êâæ

        # === ÂÖ®ÂúñÊúâÊïàÊ∑±Â∫¶ÈªûÂ∫ßÊ®ô ===
        h, w = self.depth_image.shape
        all_coords = [(x, y) for y in range(h) for x in range(w) if self.depth_image[y, x] > 0]

        # === Âæû all_coords ‰∏≠ÊéíÈô§ foreground ‚Üí ÂæóÂà∞ËÉåÊôØÂÉèÁ¥† ===
        background_coords = [np.array([x, y]) for (x, y) in all_coords if (x, y) not in foreground_coords_set]

        if background_coords:
            background_coords = np.array(background_coords)
            bg_pts3d = self.pixel_to_3d_points(background_coords, self.depth_image, self.intrinsics)
            all_instance_points.append(bg_pts3d)

            bg_color = [0.5, 0.5, 0.5]  # ËÉåÊôØÁµ¶ÁÅ∞Ëâ≤
            all_instance_draw_colors.extend([bg_color] * len(bg_pts3d))

            bg_original_colors = self.color_image[background_coords[:, 1], background_coords[:, 0]] / 255.0
            all_instance_original_colors.extend(bg_original_colors)

        # === ËôïÁêÜÊØèÂÄãÂØ¶‰æãÁöÑÈªûÈõ≤ ===
        for i, pix2d in enumerate(self.mask_pixel_list):
            valid_pix2d = pix2d[self.depth_image[pix2d[:, 1], pix2d[:, 0]] > 0]  # ÂéªÈô§Ê∑±Â∫¶ÁÑ°ÊïàÈªû
            pts3d = self.pixel_to_3d_points(valid_pix2d, self.depth_image, self.intrinsics)
            all_instance_points.append(pts3d)

            class_name = self.class_id_list[i]
            class_index = next((k for k, v in class_names_dict.items() if v == class_name), None)
            color = class_colors.get(class_index, (255, 255, 255))
            normalized_color = [c / 255.0 for c in color]
            all_instance_draw_colors.extend([normalized_color] * len(pts3d))

            original_colors = self.color_image[valid_pix2d[:, 1], valid_pix2d[:, 0]] / 255.0
            all_instance_original_colors.extend(original_colors)

        # === ÊúÄÁµÇÁµêÊûú‰øùÂ≠ò ===
        self.all_instance_points = all_instance_points
        self.all_instance_original_colors = all_instance_original_colors
        self.all_instance_draw_colors = all_instance_draw_colors
        return all_instance_points, all_instance_original_colors, all_instance_draw_colors

    
    def generate_o3d_pointcloud(self):
        np_all_instance_points = np.vstack(self.all_instance_points)
        np_all_instance_original_colors = np.array(self.all_instance_original_colors)[:,::-1]
        np_all_instance_draw_colors = np.array(self.all_instance_draw_colors)[:,:]
        pcd=o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(np_all_instance_points)
        pcd.colors = o3d.utility.Vector3dVector(np_all_instance_draw_colors)  # Ë®≠ÂÆöÈªûÈõ≤È°èËâ≤
        # pcd.colors = o3d.utility.Vector3dVector(np_all_instance_original_colors)
        return pcd      

    def get_mask_data_accurate(self,image_src):
        def roi_cut(color_image, pt1:tuple=(0,0),pt2:tuple=(1280,720)):
            x1, y1 = pt1
            x2, y2 = pt2

            # === ÂàáÂâ≤ ROI ===
            color_image = color_image[y1:y2, x1:x2]
            
            return color_image,pt1

        image=image_src.copy()
        image,pt1=roi_cut(image,pt1=(230,10),pt2=(1040,700))
        
        results = model.predict(
            source=image,
            verbose=False,
            save=False,
            retina_masks=True,  # ‚Üê ‚úÖ Á¢∫‰øùËº∏Âá∫È´òËß£ÊûêÂ∫¶ÈÅÆÁΩ©
            device='cpu'
        )
        x1,y1=pt1
        # image = cv2.imread(image_path)
        h, w = image.shape[:2]

        mask_pixel_list = []
        class_id_list = []
        confidence_score_list = []

        for result in results:
            masks = result.masks.data  # [n, h, w] torch.Tensor
            classes = result.boxes.cls.cpu().numpy()
            scores = result.boxes.conf.cpu().numpy()

            for i, (cls, conf) in enumerate(zip(classes, scores)):
                class_name = class_names_dict.get(int(cls), "Unknown")
                class_id_list.append(class_name)
                confidence_score_list.append(conf)

                # ÊèêÂèñÂñÆ‰∏Ä mask
                mask_tensor = masks[i]  # shape: (h, w)
                mask = mask_tensor.cpu().numpy().astype(np.uint8) * 255
                mask_resized = cv2.resize(mask, (w, h), interpolation=cv2.INTER_NEAREST)

                # Áç≤ÂèñÈÅÆÁΩ©ÂÉèÁ¥†Èªû‰ΩçÁΩÆ
                y, x = np.where(mask_resized > 0)
                y=y+y1
                x=x+x1
                pixel_coords = np.stack([x, y], axis=1)
                mask_pixel_list.append(pixel_coords)

                # ÈÅÆÁΩ©‰∏äËâ≤ËàáÁñäÂä†
                color = class_colors.get(int(cls), (255, 255, 255))
                overlay = image.copy()
                contours, _ = cv2.findContours(mask_resized, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                cv2.drawContours(overlay, contours, -1, color, thickness=cv2.FILLED)
                image = cv2.addWeighted(overlay, 0.5, image, 0.5, 0)

                # ÈÇäÊ°ÜËàáÊñáÂ≠ó
                cv2.drawContours(image, contours, -1, color, thickness=2)
                for contour in contours:
                    M = cv2.moments(contour)
                    if M["m00"] > 0:
                        cX = int(M["m10"] / M["m00"])
                        cY = int(M["m01"] / M["m00"])
                        label = f"{class_name} {conf:.2f}"
                        cv2.putText(image, label, (cX, cY), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

        return image, mask_pixel_list, class_id_list, confidence_score_list 
        
def preprocess_point_cloud(pcd, voxel_size=0.003)->o3d.geometry.PointCloud:
    pcd_down = pcd.voxel_down_sample(voxel_size)
    pcd_down.estimate_normals(
        search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2, max_nn=30)
    )
    return pcd_down

def icp_align_origin(source, target, threshold=0.02):
    result = o3d.pipelines.registration.registration_icp(
        source, target, threshold, np.eye(4),
        o3d.pipelines.registration.TransformationEstimationPointToPoint()
    )
    return result.transformation

def icp_align_robust_plane(source, target, threshold=0.01, sigma=0.01):
    loss = o3d.pipelines.registration.TukeyLoss(k=sigma)
    estimator = o3d.pipelines.registration.TransformationEstimationPointToPlane(loss)
    result = o3d.pipelines.registration.registration_icp(
        source, target, threshold, np.identity(4), estimator
    )
    return result.transformation, result

def o3d_to_ros_pointcloud(pcd, frame_id="base"):
    # Â∞á Open3D ÁöÑÈªûÈõ≤ËΩâÁÇ∫ PointCloud2 ROS Ë®äÊÅØ
    points = np.asarray(pcd.points)
    colors = (np.asarray(pcd.colors) * 255).astype(np.uint8)
    data = []

    for i in range(len(points)):
        x, y, z = points[i]
        r, g, b = colors[i]
        rgb = int(r) << 16 | int(g) << 8 | int(b)
        data.append([x, y, z, rgb])

    header = Header()
    header.stamp = rospy.Time.now()
    header.frame_id = frame_id

    fields = [
        PointField('x', 0, PointField.FLOAT32, 1),
        PointField('y', 4, PointField.FLOAT32, 1),
        PointField('z', 8, PointField.FLOAT32, 1),
        PointField('rgb', 12, PointField.UINT32, 1),
    ]

    pc2_msg = pcl2.create_cloud(header, fields, data)
    return pc2_msg

def post_process(depth_frame):
    # === Disparity transformation ===
    depth_to_disparity = rs.disparity_transform(True)
    disparity_to_depth = rs.disparity_transform(False)
    depth_frame = depth_to_disparity.process(depth_frame)

    # === Decimation Filter ===
    decimation = rs.decimation_filter()
    decimation.set_option(rs.option.filter_magnitude, 1)
    depth_frame = decimation.process(depth_frame)

    # === Spatial Filter ===
    spatial = rs.spatial_filter(
        smooth_alpha=0.25, 
        smooth_delta=50.0, 
        magnitude=3.0,
        hole_fill=2.0
    )
    depth_frame = spatial.process(depth_frame)

    # === Temporal Filter ===
    temporal = rs.temporal_filter(0.25, 100.0, 8)  # alpha, delta, persistency_mode
    depth_frame = temporal.process(depth_frame)

    # === Convert back to depth ===
    depth_frame = disparity_to_depth.process(depth_frame)

    # === Hole Filling ===
    # hole_filling = rs.hole_filling_filter()
    # hole_filling.set_option(rs.option.holes_fill, 1)
    # depth_frame = hole_filling.process(depth_frame)

    return depth_frame


# ÂÆöÁæ©ËΩâÁßªÁü©Èô£ÁöÑÂáΩÊï∏
def R_and_t_to_T(R, t):
    T = np.hstack((R, t.reshape(-1, 1)))  # Â∞áÂπ≥ÁßªÂêëÈáèËΩâÊèõÁÇ∫ÂàóÂêëÈáè
    T = np.vstack((T, [0, 0, 0, 1]))  # Ê∑ªÂä†ÊúÄÂæå‰∏ÄË°å
    return T

def T_to_R_and_t(T):
    Rt = T[:3]
    R = Rt[:, :3]
    t = Rt[:, 3].reshape((-1, 1))
    return R, t
  
def collision_detect_z(position, approach, binormal, axis, bbox_id,
                                   hand_depth=0.07, hand_height=0.02,
                                   outer_diameter=0.105, finger_width=0.01,
                                   table_z=0.08828, frame_id="base"):
    hw = 0.5 * outer_diameter - 0.5 * finger_width
    R = np.column_stack((approach, binormal, axis))
    z_compensation = 0.0
    z_compensation_mm=0.0
    left_center = position - hw * binormal + 0.5 * hand_depth * approach
    right_center = position + hw * binormal + 0.5 * hand_depth * approach
    left_tip = position - hw * binormal + hand_depth * approach
    right_tip = position + hw * binormal + hand_depth * approach

    # markers = MarkerArray()

    # # left & right box
    # for idx, center in enumerate([left_center, right_center]):
    #     marker = Marker()
    #     marker.header.frame_id = frame_id
    #     marker.header.stamp = rospy.Time.now()
    #     marker.ns = "bbox"
    #     marker.id = bbox_id * 10 + idx
    #     marker.type = Marker.CUBE
    #     marker.action = Marker.ADD
    #     marker.pose.position.x, marker.pose.position.y, marker.pose.position.z = center
    #     quat = R_scipy.from_matrix(R).as_quat()
    #     marker.pose.orientation.x, marker.pose.orientation.y, marker.pose.orientation.z, marker.pose.orientation.w = quat
    #     marker.scale.x, marker.scale.y, marker.scale.z = hand_depth, finger_width, hand_height
    #     marker.color.r, marker.color.g, marker.color.b, marker.color.a = 0.0, 1.0, 0.0, 0.4
    #     marker.lifetime = rospy.Duration(30)
    #     markers.markers.append(marker)

    # fingertips
    # for idx, tip in enumerate([left_tip, right_tip]):
    #     marker = Marker()
    #     marker.header.frame_id = frame_id
    #     marker.header.stamp = rospy.Time.now()
    #     marker.ns = "fingertip"
    #     marker.id = bbox_id * 10 + 2 + idx
    #     marker.type = Marker.SPHERE
    #     marker.action = Marker.ADD
    #     marker.pose.position.x, marker.pose.position.y, marker.pose.position.z = tip
    #     marker.scale.x = marker.scale.y = marker.scale.z = 0.01
    #     marker.color.r, marker.color.g, marker.color.b, marker.color.a = 1.0, 0.0, 0.0, 1.0
    #     marker.lifetime = rospy.Duration(30)
    #     markers.markers.append(marker)

    # Á¢∞ÊíûÊ™¢Êü•
    tip_z_min = min(left_tip[2], right_tip[2])
    is_danger = tip_z_min < table_z
    if is_danger:
            z_compensation = table_z+0.005- tip_z_min
            z_compensation_mm = z_compensation * 1000  # ËΩâÊèõÁÇ∫ mm
            rospy.logwarn(f"‚ö†Ô∏è ÊåáÂ∞ñÁ¢∞ÊíûÊ™¢Ê∏¨Ôºötip_z_min={tip_z_min} , ÈúÄË¶ÅË£úÂÑü {z_compensation_mm:.3f} mm")
            
    
    return z_compensation_mm, is_danger

def classify_grasp_color_by_counts(colors_in_box, class_colors, class_names_dict):
    """
    Ê†πÊìöÈªûÈõ≤‰∏≠ÁöÑÈ°èËâ≤Áµ±Ë®àÔºåÊ±∫ÂÆöÂì™‰∏ÄÈ°ûÈ°èËâ≤Âú®Ë©≤ grasp ‰∏≠Âç†ÊØîÊúÄÈ´ò„ÄÇ
    :param colors_in_box: (N, 3) numpy array of float RGB values [0,1]
    :param class_colors: dict[class_id] = (R, G, B) 255 scale
    :param class_names_dict: dict[class_id] = "name"
    :return: class_name (str)
    """
    counts = defaultdict(int)

    # Normalize class colors
    class_colors_normalized = {
        class_id: np.array(rgb) / 255.0 for class_id, rgb in class_colors.items()
    }

    for color in colors_in_box:
        for class_id, class_color in class_colors_normalized.items():
            if np.allclose(color, class_color, atol=0.1):
                counts[class_id] += 1
                break

    if counts:
        # ÊâæÂá∫ÊúÄÂ§öÁ•®ÁöÑÈ°ûÂà•
        max_class_id = max(counts.items(), key=lambda x: x[1])[0]
        return class_names_dict.get(max_class_id, "unknown")
    else:
        return "unknown"

def grasp_callback(msg,combined_pcd):
    # global T_grasps_in_base_list,T_grasps_in_base_list,T_tool_list  # ‰ΩøÁî®ÂÖ®ÂüüËÆäÊï∏
    
    
    T_grasps_in_base_list = []
    T_tool_list=[]
    gripper_base_position_mm_in_base_list= []
    tcp_pos_mm_in_base_list = []
    class_in_base_list = []
    
    rospy.loginfo("Êî∂Âà∞ %d ÂÄãÊäìÂèñÂßøÊÖã", len(msg.grasps))
    if len(msg.grasps) !=0:
        have_grasps=True
        for i, grasp in enumerate(msg.grasps):
            # Â∞áÊäìÂèñÂßøÊÖãÁöÑË≥áË®äËΩâÊèõÁÇ∫ np.array
            gripper_base_position = np.array([grasp.position.x, grasp.position.y, grasp.position.z])
            approach = np.array([grasp.approach.x, grasp.approach.y, grasp.approach.z])
            binormal = np.array([grasp.binormal.x, grasp.binormal.y, grasp.binormal.z])
            axis = np.array([grasp.axis.x, grasp.axis.y, grasp.axis.z])
            sample = np.array([grasp.sample.x, grasp.sample.y, grasp.sample.z]) 
            
            # === Êé®ÁÆó TCP ‰∏ñÁïåÂ∫ßÊ®ô ===
            hand_depth = 0.07        # 7cm Â§æÁà™Ê∑±Â∫¶
            deepen_offset = 0.005     # Ëã•ÈñãÂïü deepen_handÔºåÈÄôË£°ÊáâË®≠ÁÇ∫ 0.01
            weight=1 # Â§æÁà™Ê∑±Â∫¶ÁöÑÊ¨äÈáçÔºåËã•Êúâ TCP_frame_transformÔºåÂâáË®≠ÁÇ∫ 1
            tcp_offset = hand_depth / weight - deepen_offset
            tcp_position = gripper_base_position + tcp_offset * approach
            tcp_position_long = gripper_base_position + (hand_depth ) * approach  # Èï∑Â∫¶ÊñπÂêëÁöÑ TCP ‰ΩçÁΩÆ
            
            # Ë®àÁÆóÂåÖÂúçÈï∑ÊñπÈ´îÁöÑÁØÑÂúç
            box_length = np.linalg.norm(tcp_position_long - gripper_base_position+0.01)  # Èï∑ÊñπÈ´îÁöÑÈï∑Â∫¶
            box_width = 20 / 1000  # Èï∑ÊñπÈ´îÁöÑÂØ¨Â∫¶Ôºà0.02mmÔºâ
            box_height = 20 / 1000  # Èï∑ÊñπÈ´îÁöÑÈ´òÂ∫¶Ôºà0.02mmÔºâ
            # ÈÅçÊ≠∑ÈªûÈõ≤ÔºåÁØ©ÈÅ∏Âá∫Âú®ÂåÖÂúçÈï∑ÊñπÈ´îÂÖßÁöÑÈªû

            points_in_box = []
            colors_in_box = []
            for point, color in zip(combined_pcd.points, combined_pcd.colors):
                # Ë®àÁÆóÈªûÂà∞ position ÁöÑÂêëÈáè
                vector_to_point = np.array(point) - gripper_base_position

                # Ë®àÁÆóÂú® approach, binormal, axis ‰∏äÁöÑÊäïÂΩ±
                proj_length = np.dot(vector_to_point, approach)
                proj_width = np.dot(vector_to_point, binormal)
                proj_height = np.dot(vector_to_point, axis)

                # Âà§Êñ∑ÈªûÊòØÂê¶Âú®ÂåÖÂúçÈï∑ÊñπÈ´îÂÖß
                if (0 <= proj_length <= box_length and
                    -box_width / 2 <= proj_width <= box_width / 2 and
                    -box_height / 2 <= proj_height <= box_height / 2):
                    points_in_box.append(point)
                    colors_in_box.append(color)

            # Áµ±Ë®àÁØÑÂúçÂÖßÁöÑ‰∏ªË¶ÅÈ°èËâ≤
            if colors_in_box:
                colors_in_box = np.array(colors_in_box)
                avg_color = np.mean(colors_in_box, axis=0)  # Ë®àÁÆóÂπ≥ÂùáÈ°èËâ≤
                # print(f"ÊäìÂèñÂßøÊÖã {i} ÁöÑ‰∏ªË¶ÅÈ°èËâ≤: {avg_color}")

                # Âà§Êñ∑Áâ©È´îÈ°ûÂûã
                for class_id, class_color in class_colors.items():
                    normalized_color = np.array(class_color) / 255.0
                    if np.allclose(avg_color, normalized_color, atol=0.1):  # È°èËâ≤Áõ∏‰ººÂà§Êñ∑
                        # print(f"ÊäìÂèñÂßøÊÖã {i} ÁöÑÁâ©È´îÈ°ûÂûã: {class_names_dict[class_id]}")
                        class_in_base_list.append(class_names_dict[class_id])
                        break
                class_in_base_list.append('unknown')  # Ëã•Ê≤íÊúâÂåπÈÖçÂà∞‰ªª‰ΩïÈ°ûÂà•ÔºåÂâáÊ®ôË®òÁÇ∫ unknown
                
            else:
                print(f"ÊäìÂèñÂßøÊÖã {i} ÁöÑÁØÑÂúçÂÖßÊ≤íÊúâÈªûÈõ≤")
                class_in_base_list.append(None)

            # === Âª∫Á´ã grasp frame ===
            R_grasp = np.column_stack((approach, binormal, axis))

            # Âª∫Á´ãËΩâÁßªÁü©Èô£
            T_tcp = R_and_t_to_T(R_grasp, tcp_position) 
            
            T_grasp_in_base = T_tcp  # ÈÄôÂ∞±ÊòØÊäìÂèñÂßøÊÖãÁöÑ TCP

            T_grasps_in_base_list.append(T_grasp_in_base)

            # Ëã•Ë¶ÅÂàÜÈõ¢Âá∫ÊóãËΩâËàáÂπ≥ÁßªÈÉ®ÂàÜÔºàÂèØÈÅ∏Ôºâ
            # R_grasp_in_base,t_grasp_in_base = T_to_R_and_t(T_grasp_in_base)


            # === Â∞á TCP ÂÜçËΩâÁÇ∫Â∑•ÂÖ∑Â∫ßÊ®ôÁ≥ªÔºàËã•Êúâ TCP_frame_transformÔºâ ===
            TCP_frame_transform = np.array([
                [0, 1, 0, 0],
                [0, 0, 1, 0],
                [1, 0, 0, 0],
                [0, 0, 0, 1]
            ])
            TCP_frame_transform_inv = np.linalg.inv(TCP_frame_transform)
            T_tool = T_grasp_in_base @ TCP_frame_transform_inv
            T_tool_list.append(T_tool)

            # ÂàÜËß£ÂßøÊÖã
            R_tool, t_tool = T_to_R_and_t(T_tool)
            euler_angles = R_scipy.from_matrix(R_tool).as_euler('xyz', degrees=True)
            rx, ry, rz = euler_angles

            # Z Ëª∏ÊñπÂêëÁøªËΩâË£úÊ≠£ÔºàËã• rz ÁÇ∫Ë≤†Ôºâ
            if rz < 0:
                R_flip = R_scipy.from_euler('z', 180, degrees=True).as_matrix()
                T_flip = np.eye(4)
                T_flip[:3, :3] = R_flip
                T_tool = T_tool @ T_flip
                R_tool, t_tool = T_to_R_and_t(T_tool)
                rx, ry, rz = R_scipy.from_matrix(R_tool).as_euler('xyz', degrees=True)

            # TCP ÈªûËàá sample Èªû‰ª• mm ÁÇ∫ÂñÆ‰ΩçËº∏Âá∫
            tcp_pos_mm = t_tool * 1000
            gripper_base_position_mm = gripper_base_position * 1000
            x0, y0, z0 = gripper_base_position_mm
            x, y, z = [float(v) for v in tcp_pos_mm]
            
            z_compensation_mm, is_danger= collision_detect_z(
                gripper_base_position, approach, binormal, axis, bbox_id=i,
                hand_depth=0.07, hand_height=0.02,
                outer_diameter=0.105, finger_width=0.01,
                table_z=0.08, frame_id="base"
            )
        
            if is_danger:
                # rospy.logwarn(f"[Grasp {i}] ‚ö†Ô∏è ÊåáÂ∞ñÂèØËÉΩÁ¢∞ÊíûÊ°åÈù¢ÔºÅ\n"
                #               f"origin_gripper_base_position x={x0:.2f}, y={y0:.2f}, z={z0:.2f}, rx={rx:.2f}, ry={ry:.2f}, rz={rz:.2f}\n"
                #               f"origin_TCP x={x:.2f}, y={y:.2f}, z={z:.2f}, rx={rx:.2f}, ry={ry:.2f}, rz={rz:.2f}\n"
                #               )
                z0 += z_compensation_mm
                z  += z_compensation_mm
            # ÂÑ≤Â≠òÁµêÊûú
            gripper_base_position_mm_in_base_list.append([x0, y0, z0, rx, ry, rz])
            tcp_pos_mm_in_base_list.append([x, y, z, rx, ry, rz])
            # Ëº∏Âá∫È°ØÁ§∫
        #     color_print(f'[gripper_base_position ] x={x0:.2f}, y={y0:.2f}, z={z0:.2f}, rx={rx:.2f}, ry={ry:.2f}, rz={rz:.2f}', color='cyan')
        #     color_print(f'[TCP] x={x:.2f}, y={y:.2f}, z={z:.2f}, rx={rx:.2f}, ry={ry:.2f}, rz={rz:.2f}', color='cyan')

        color_print(f'Best_gripper_base_position = {gripper_base_position_mm_in_base_list[0]}', color='green')
        color_print(f'Best_TCP_position = {tcp_pos_mm_in_base_list[0]}', color='green')
        print('len =', len(T_grasps_in_base_list))

    else:
        rospy.logwarn("Ê≤íÊúâÊî∂Âà∞‰ªª‰ΩïÊäìÂèñÂßøÊÖãÔºåË´ãÊ™¢Êü•Ëº∏ÂÖ•Ë®äÊÅØ")
        have_grasps = False
        return [], [], [],have_grasps 
    return gripper_base_position_mm_in_base_list, tcp_pos_mm_in_base_list, class_in_base_list,have_grasps


def color_str(text, color="white"):
    color_codes = {
        "black":   "\033[30m",
        "red":     "\033[31m",
        "green":   "\033[32m",
        "yellow":  "\033[33m",
        "blue":    "\033[34m",
        "magenta": "\033[35m",
        "cyan":    "\033[36m",
        "white":   "\033[37m"
    }
    reset_code = "\033[0m"

    color = color.lower()
    if color not in color_codes:
        print(f"\033[31m[ÈåØË™§] ‰∏çÊîØÊè¥ÁöÑÈ°èËâ≤Ôºö{color}\033[0m")
        return f"{color_codes[color]}{text}{reset_code}"

    

def color_print(text, color="white"):
    color_codes = {
        "black":   "\033[30m",
        "red":     "\033[31m",
        "green":   "\033[32m",
        "yellow":  "\033[33m",
        "blue":    "\033[34m",
        "magenta": "\033[35m",
        "cyan":    "\033[36m",
        "white":   "\033[37m"
    }
    reset_code = "\033[0m"

    color = color.lower()
    if color not in color_codes:
        print(f"\033[31m[ÈåØË™§] ‰∏çÊîØÊè¥ÁöÑÈ°èËâ≤Ôºö{color}\033[0m")
        return

    print(f"{color_codes[color]}{text}{reset_code}")




# ÈÅçÊ≠∑Ë≥áÊñôÂ§æ‰∏≠ÁöÑÊâÄÊúâÂúñÁâá
def get_image_path_list(test_image_path = r"/home/chen/Segmentation_Train/train_data/test/images"):
    # Á¢∫‰øùÊ∏¨Ë©¶ÂúñÁâáÂ≠òÂú®
    if not os.path.exists(test_image_path):
        raise FileNotFoundError(f"Ê∏¨Ë©¶ÂúñÁâá‰∏çÂ≠òÂú®: {test_image_path}")
    img_list=[]
    for image_name in os.listdir(test_image_path):
        # Á¢∫‰øùÊòØÂúñÁâáÊ™îÊ°à
        if image_name.lower().endswith(('.jpg', '.jpeg', '.png')):
            image_path = os.path.join(test_image_path, image_name)
            img_list.append(image_path)
            # print(f"Ê≠£Âú®ËôïÁêÜÂúñÁâá: {image_path}")
    return img_list

if __name__ == "__main__":
    rospy.init_node('mask_point_cloud', anonymous=True)
    cloud_publisher=rospy.Publisher('cloud_stitched', PointCloud2, queue_size=10)

    # gripper_pick(pick_distance=0,inital_bool=False)
    change_tcp("ChangeTCP(\"robotiq_origin_gripper\")")
    

    VIEW_POSE_1=[464.74 , -199.48 , 381.93 , 146.32 , 4.39 , 165.82] #TCP
    VIEW_POSE_2=[465.60 , 56.86 , 409.25 , 145.48 , 2.77 , 1.23] #TCP
    VIEW_POSE_3=[355.23 , -88.64 , 364.20 , 144.28 , -0.96 , 94.84] #TCP
    VIEW_POSE_LIST=[VIEW_POSE_1,VIEW_POSE_2,VIEW_POSE_3]
    PREPARE_POSE=[552.75 , -299.90 , 332.59 , 146.32 , 4.39 , 90.84] #TCP
    PUT_POSE_0=[655.84 , 499.26 , 239.60 , -180.00 , 0.00 , 180] #TCP
    PUT_POSE_1=[459.60 , 499.26 , 239.60 , -180.00 , 0.00 , 180] #TCP
    PUT_POSE_2=[252.89 , 499.26 , 239.60 , -180.00 , 0.00 , 180] #TCP
    RECYCLE_POSE=[-35.03 , 776.64 , 297.22, 180.00 ,0.00 ,-180.00]#TCP
    
    PUT_DOWN_POSE_0=[655.84 , 499.26 , 207.27 , -180.00 , 0.00 , 180] #TCP
    PUT_DOWN_POSE_1=[459.60 , 499.26 , 207.27 , -180.00 , 0.00 , 180] #TCP
    PUT_DOWN_POSE_2=[252.89 , 499.26 , 207.27 , -180.00 , 0.00 , 180] #TCP
    # PUT_DOWN_RECYCLE_POSE=[-35.03 , 776.64 , 183.32, 180.00 ,0.00 ,-180.00]
    move_script_with_monitor(RECYCLE_POSE)
    # move_script_with_monitor(PUT_POSE_2[:2]+[161.9]+PUT_POSE_2[3:])
    # move_script_with_monitor(PUT_DOWN_RECYCLE_POSE)
    gripper_pick(pick_distance=0,inital_bool=False)
    move_script_with_monitor(RECYCLE_POSE)
    input('stop')
    # Create a pipeline
    pipeline = rs.pipeline()
    # Create a config and configure the pipeline to stream
    #  different resolutions of color and depth streams
    CONFIG = rs.config()
    CONFIG.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 30)
    CONFIG.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)
    # Start streaming
    profile = pipeline.start(CONFIG)
    device = profile.get_device()

    # === Apply JSON Advanced Settings ===
    try:
        advanced_mode = rs.rs400_advanced_mode(device)
        if not advanced_mode.is_enabled():
            print("üõ† Ê≠£Âú®ÂïüÁî® advanced mode ...")
            advanced_mode.toggle_advanced_mode(True)
            import time
            time.sleep(1)
        # Â•óÁî® JSON
        with open('/home/chen/realsense_setting/realsesnse_setting3.json', 'r') as file:
            json_text = file.read()
            advanced_mode.load_json(json_text)
        print("‚úÖ JSON Ë®≠ÂÆöÂ∑≤ÊàêÂäüËºâÂÖ•")
    except Exception as e:
        print("‚ö†Ô∏è Ë£ùÁΩÆ‰∏çÊîØÊè¥ advanced_modeÔºåÊàñÂàùÂßãÂåñÂ§±ÊïóÔºö", e)


    align_to = rs.stream.color
    align = rs.align(align_to)

    ply_number=0
    while not rospy.is_shutdown():
    # Streaming loop
        view_point_cloud_list=[]
        move_script_with_monitor(VIEW_POSE_3,motion_type='PTP')
        for i, view_pose in enumerate(VIEW_POSE_LIST):
            # input(f"\nüì∏ ÁßªÂãïÂà∞Á¨¨ {i+1} Ë¶ñËßíÂæåÊåâ Enter Êì∑Âèñ...")
            move_script_with_monitor(view_pose,motion_type='PTP')
            
            for _ in range(10): frames = pipeline.wait_for_frames()  # Á≠âÂπæÂπÄËÆìË≥áÊñôÁ©©ÂÆö
            # Align the depth frame to color frame
            aligned_frames = align.process(frames)

            # Get aligned frames
            aligned_depth_frame = aligned_frames.get_depth_frame()  # aligned_depth_frame is a 640x480 depth image
            color_frame = aligned_frames.get_color_frame()

            # ÊáâÁî® Hole Filling Filter
            filtered_depth_frame = post_process(aligned_depth_frame)

            # Â∞áÊ∑±Â∫¶ÂΩ±ÂÉèËΩâÊèõÁÇ∫ NumPy Èô£Âàó
            depth_image = np.asanyarray(filtered_depth_frame.get_data())
            color_image = np.asanyarray(color_frame.get_data())
            
            cv2.imwrite(os.path.join(image_path_test, f'{i}_detect.png'), color_image)
            # exit()
            view_point_cloud_list.append(
                view_point_cloud(i,
                                color_image,
                                depth_image,
                                profile.get_stream(rs.stream.color).as_video_stream_profile().get_intrinsics()))
            
                    # È†êËôïÁêÜÈªûÈõ≤
        # processed_point_clouds = [preprocess_point_cloud(vpc.o3d_pcd) for vpc in view_point_cloud_list]
        processed_point_clouds = [vpc.o3d_pcd for vpc in view_point_cloud_list]
        
        # Ë®≠ÂÆöÂü∫Ê∫ñÈªûÈõ≤
        target = processed_point_clouds[0]
        combined_pcd = target  # ÂàùÂßãÂåñÂêà‰ΩµÈªûÈõ≤
        
        # ÁñäÂêàÂÖ∂‰ªñÈªûÈõ≤Âà∞Âü∫Ê∫ñÈªûÈõ≤
        for i in range(1, len(processed_point_clouds)):
            source = processed_point_clouds[i]
            print(f"Ê≠£Âú®ÈÄ≤Ë°åÁ¨¨ {i} ÂÄãÈªûÈõ≤ÁöÑ ICP ÁñäÂêà...")
            
            # # ‰ΩøÁî® icp_align_robust_plane ÈÄ≤Ë°åÁñäÂêà
            # transformation, result = icp_align_robust_plane(source, target, threshold=0.02, sigma=0.05)
            # print(f"ICP ÁñäÂêàÂÆåÊàêÔºåËΩâÊèõÁü©Èô£Ôºö\n{transformation}")
            
            # # Â∞áËΩâÊèõÁü©Èô£ÊáâÁî®Âà∞ÈªûÈõ≤
            # source.transform(transformation)
            
            # Âêà‰ΩµÈªûÈõ≤
            combined_pcd += source
        
        points = np.asarray(combined_pcd.points)
        # # Ë®≠ÂÆö Z ÂÄº‰∏ãÈôê
        # z_threshold = 0.08828
        # # Âª∫Á´ãÂ∏ÉÊûóÈÅÆÁΩ©ÔºåÂè™‰øùÁïô Z ÂÄº >= z_threshold ÁöÑÈªû
        # mask = points[:, 2] >= z_threshold
        # # Â•óÁî®ÈÅÆÁΩ©‰∏¶Âª∫Á´ãÊñ∞ÁöÑÈªûÈõ≤Áâ©‰ª∂
        # filtered_pcd = combined_pcd.select_by_index(np.where(mask)[0])
        
        # Ë®≠ÂÆö X, Y, Z ÁöÑÁØÑÂúç
        x_min, x_max = 0.241, 0.680
        y_min, y_max = -0.33989, 0.16684
        # z_min,z_max =0.08828 ,0.26677  # Âè™Êúâ‰∏ãÈôê
        z_min,z_max =0.08228 ,0.26677  # Âè™Êúâ‰∏ãÈôê
        # Âª∫Á´ãÂ∏ÉÊûóÈÅÆÁΩ©
        mask = (
            (points[:, 0] >= x_min) & (points[:, 0] <= x_max) &  # X ÁØÑÂúç
            (points[:, 1] >= y_min) & (points[:, 1] <= y_max) &  # Y ÁØÑÂúç
            (points[:, 2] >= z_min) & (points[:, 1] <= z_max))    # Z ÁØÑÂúç
        
        
        # Ê†πÊìöÈÅÆÁΩ©ÈÅ∏ÂèñÁ¨¶ÂêàÊ¢ù‰ª∂ÁöÑÈªû
        filtered_indices = np.where(mask)[0]
        filtered_pcd = combined_pcd.select_by_index(filtered_indices)
        
        
        
        combined_pcd = filtered_pcd   
        # cl, ind = combined_pcd.remove_statistical_outlier(nb_neighbors=200, std_ratio=2.0)
        
        # combined_pcd=combined_pcd.select_by_index(ind)  
            
                
        # Âª∫Á´ãÂ∫ßÊ®ôÁ≥ª
        axis = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1, origin=[0, 0, 0])
        
        
        # Ë®≠ÂÆöË¶ñËßíÂèÉÊï∏
        zoom = 2
        
        front = [-0.5, 0.0, 0.0]
        lookat = [0.0, 0.0, 0.0]
        up = [0.0, 0.0, 1.0]

        # msg=o3d_to_ros_pointcloud(pcd_combined)
        o3d.io.write_point_cloud("/home/chen/catkin_ws/src/pcl_with_gpd/ply_save/"+f"{ply_number}.ply", combined_pcd, write_ascii=True)
        ply_number+=1
        cloud_publisher.publish(o3d_to_ros_pointcloud(combined_pcd))
        # È°ØÁ§∫ÈªûÈõ≤ÂíåÂ∫ßÊ®ôÁ≥ª
        geometries_to_draw = [combined_pcd] + [axis]
        
        '''o3d.visualization.draw_geometries(geometries_to_draw,
                                        zoom=zoom,
                                        front=front,
                                        lookat=lookat,
                                        up=up,
                                        window_name="RGB PointCloud from RealSense") 
                # Display the color image with ROI
            # cv2.imshow('Color Image with ROI', color_image)

            # # Display the point cloud
            # if input("go")=="go":
            #     o3d.visualization.draw_geometries([pcd])

            # key = cv2.waitKey(0)
            # if key & 0xFF == ord('q'):
            #     break
            # elif key & 0xff== ord('m'):
            #     o3d.visualization.draw_geometries([pcd])
            #     continue
                                        '''
        voxelize_pcd=combined_pcd.voxel_down_sample(voxel_size=0.005)  # Â∞áÈªûÈõ≤ÈÄ≤Ë°åÈ´îÁ¥†ÂåñËôïÁêÜ
        # bbox_array_pub = rospy.Publisher("/grasp_bbox_array", MarkerArray, queue_size=1)
            
        print("Á≠âÂæÖÂßøÊÖãÊ∂àÊÅØ...")
        have_grasps=False
        
        while  not rospy.is_shutdown():
            # color_print("Ê≤íÊúâÊäìÂèñÂßøÊÖãÔºåË´ãÊ™¢Êü•Ëº∏ÂÖ•Ë®äÊÅØ", "red")
            # continue
            msg = rospy.wait_for_message("/detect_grasps/clustered_grasps", GraspConfigList,60)  # Á≠âÂæÖÊé•Êî∂ÊäìÂèñÂßøÊÖãÊ∂àÊÅØ

            position_in_base_list,sample_in_base_list,class_in_base_list,have_grasps=grasp_callback(msg,voxelize_pcd)  # ËôïÁêÜÊé•Êî∂Âà∞ÁöÑÊ∂àÊÅØ 
            if input("ÊòØÂê¶Èáç‰æÜÔºü(y/n): ").strip().lower() == 'y':
                cloud_publisher.publish(o3d_to_ros_pointcloud(combined_pcd))
                color_print("ÁôºÈÄÅÈªûÈõ≤", "blue")
            else:
                break
            # ros_cloud=o3d_to_ros_pointcloud(pcd = o3d.io.read_point_cloud("/home/chen/catkin_ws/src/pcl_with_gpd/scripts/test.ply") , frame_id="base")
            
        
        
        for position ,sample,classes in zip(position_in_base_list,sample_in_base_list,class_in_base_list):
            if classes is not None:
                
                best_position= position
                best_sample= sample
                best_class=classes 
                print(f" best_class={best_class}")
                break
            else: color_print("ÁÑ°Ê≥ïÂàÜÈ°ûÔºåÂ∞ãÊâæ‰∏ã‰∏ÄÂÄãÂßøÊÖãÔºÅ", "yellow") 

        # exit()

        # move_script_with_monitor(PREPARE_POSE,motion_type='PTP')
        input(f'NEXT.................')
        move_script_with_monitor(best_position,motion_type='PTP')
        
        move_script_with_monitor(best_sample)
        gripper_pick(pick_distance=255,inital_bool=True)
        rospy.sleep(2)  # Á≠âÂæÖÂ§æÁà™ÈñâÂêà
        move_script_with_monitor(best_position,motion_type='PTP')
        # up_pose=best_position[:2]+[200]+best_position[3:]
        # move_script_with_monitor(up_pose)
        move_script_with_monitor(position[:2]+[385]+best_position[3:])


        obj_detect=get_gripper_position()
        if obj_detect !=2:
            
            color_print(f"obj_detect={obj_detect},üõ† Ê™¢Ê∏¨Âà∞Â§æÁà™Êú™ÂÆåÂÖ®ÈñâÂêàÔºåÊ≠£Âú®ÈñâÂêàÂ§æÁà™...", "red")
            gripper_pick(pick_distance=0,inital_bool=True)
            continue
        else:
            color_print("‚úÖ Â§æÁà™Â∑≤Á∂ìÈñâÂêà", "green")
            

        if best_class == "aluextru":
            color_print(best_class, "red")
            move_script_with_monitor(PUT_POSE_0)
            move_script_with_monitor(PUT_DOWN_POSE_0)  
            gripper_pick(pick_distance=0,inital_bool=True)
            move_script_with_monitor(PUT_POSE_0)
    
        elif best_class == "bin":
            color_print(best_class, "green")
            move_script_with_monitor(PUT_POSE_1)
            # move_script_with_monitor(PUT_POSE_1[:2]+[161.9]+PUT_POSE_1[3:])
            move_script_with_monitor(PUT_DOWN_POSE_1)
            gripper_pick(pick_distance=0,inital_bool=True)
            move_script_with_monitor(PUT_POSE_1)
            
        elif best_class == "twpipe":
            color_print(best_class, "blue")
            move_script_with_monitor(PUT_POSE_2)
            # move_script_with_monitor(PUT_POSE_2[:2]+[161.9]+PUT_POSE_2[3:])
            move_script_with_monitor(PUT_DOWN_POSE_2)
            gripper_pick(pick_distance=0,inital_bool=True)
            move_script_with_monitor(PUT_POSE_2)
        elif best_class=='unknown':
            color_print(best_class, "yellow")
            move_script_with_monitor(RECYCLE_POSE)
            # move_script_with_monitor(PUT_POSE_2[:2]+[161.9]+PUT_POSE_2[3:])
            # move_script_with_monitor(PUT_DOWN_RECYCLE_POSE)
            gripper_pick(pick_distance=0,inital_bool=True)
            move_script_with_monitor(RECYCLE_POSE)
        print("‰∏ã‰∏ÄËº™Â§æÂèñ...")
        input('next round .................')
        # cv2.destroyAllWindows()
    pipeline.stop()

