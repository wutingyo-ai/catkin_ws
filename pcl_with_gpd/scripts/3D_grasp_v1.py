#!/usr/bin/env python3.8
#####################################################
##              Align Depth to Color               ##
#####################################################
import pyrealsense2 as rs
import numpy as np
import cv2
import open3d as o3d
from ultralytics import YOLO
import os
import rospy
from tm_msgs.msg import FeedbackState
from tm_msgs.srv import AskItem, AskItemRequest
from scipy.spatial.transform import Rotation as R_scipy
import re
from sensor_msgs.msg import PointCloud2, PointField
from std_msgs.msg import Header
from visualization_msgs.msg import Marker,MarkerArray
import sensor_msgs.point_cloud2 as pcl2
from visualization_msgs.msg import MarkerArray  # å¦‚æœæœªä¾†æ”¹ç”¨ MarkerArray
from std_msgs.msg import ColorRGBA    
import math
import sys
sys.path.append('/home/chen/catkin_ws/src')  # å°‡ src åŠ å…¥æ¨¡çµ„è·¯å¾‘
from robot_control import *

# from node_control.script.listen_python_move_to_fixture import new_monitor,move_send_script
from gpd_ros.msg import GraspConfigList

class_names_dict={0: 'aluextru', 1: 'bin', 2: 'twpipe'}
class_colors = {
    0: (255, 0, 0),   # é¡åˆ¥ 0 - ç´…è‰²
    1: (0, 255, 0),   # é¡åˆ¥ 1 - ç¶ è‰²
    2: (0, 0, 255),   # é¡åˆ¥ 2 - è—è‰²
    # æ·»åŠ æ›´å¤šé¡åˆ¥é¡è‰²
}

image_path_test='/home/chen/catkin_ws/src/pcl_with_gpd/picture'
# weight_path = r"/home/chen/catkin_ws/src/pcl_with_gpd/weight/New_best.pt"
weight_path = r"/home/chen/catkin_ws/src/pcl_with_gpd/weight/2025_06_28_best.pt"
model = YOLO(weight_path)

# exit()
# print(dir(rs))
# help(rs.rs2_deproject_pixel_to_point)
class view_point_cloud:
    def __init__(self,view_id,color_image=None,depth_image=None,intrinsics=None):
        self.intrinsics=intrinsics
        self.color_image=color_image
        self.depth_image=depth_image
        self.mark_image,self.mask_pixel_list,self.class_id_list,self.confidence_score_list=self.get_mask_data_accurate(color_image)
        
        self.all_instance_points,self.all_instance_original_colors,self.all_instance_draw_colors=self.merge_mask_points()
    
        self.o3d_pcd=self.generate_o3d_pointcloud()
        
        self.view_id=view_id

    
    def extract_pose_values(self,coord_string):
        # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå–èŠ±æ‹¬è™Ÿå…§çš„å…§å®¹
        match = re.search(r'\{(.*?)\}', coord_string)
        if match:
            # å°‡æå–çš„å…§å®¹æŒ‰é€—è™Ÿåˆ†å‰²
            values_str = match.group(1).split(',')
            # å°‡å­—ç¬¦ä¸²è½‰æ›ç‚ºæµ®é»æ•¸
            values = [float(v.strip()) for v in values_str]
            return values
        else:
            return None


    def ask_flange_pose(self):
        # global robot_pose
        client = rospy.ServiceProxy('tm_driver/ask_item', AskItem)
        request = AskItemRequest()
        request.id = "demo"
        request.wait_time = 0.5
        request.item = "Coord_Robot_Flange"
        try:
            response = client(request)
            if response.ok:
                rospy.loginfo("AskItem to robot: item is {}, value is {}\n".format(request.item, response.value))
                coord_string = response.value
                pose_values = self.extract_pose_values(coord_string)
                if pose_values:
                    pose_values = np.asarray(pose_values)
                    # robot_pose=pose_values
                    return pose_values
        except rospy.ServiceException as e:
                    rospy.logerr("Error AskItem to robot: {}".format(e))


    def pose_to_matrix(self,pose, degrees=True,milimeter=True):
        """
        å°‡ä½ç½® (x, y, z) å’Œæ­æ‹‰è§’ (rx, ry, rz) è½‰æ›ç‚º 4x4 çš„é½Šæ¬¡è½‰æ›çŸ©é™£ã€‚
        
        åƒæ•¸:
            x, y, z: å¹³ç§»é‡
            rx, ry, rz: æ­æ‹‰è§’ï¼ˆå–®ä½ç‚ºå¼§åº¦æˆ–åº¦ï¼‰
            degrees: å¦‚æœç‚º Trueï¼Œå‰‡è¼¸å…¥çš„è§’åº¦ç‚ºåº¦ï¼›å¦å‰‡ç‚ºå¼§åº¦ã€‚
        
        è¿”å›:
            4x4 çš„é½Šæ¬¡è½‰æ›çŸ©é™£ï¼ˆnumpy.ndarrayï¼‰
            
        """
        x,y,z,rx,ry,rz=pose
        if degrees:
            rotation = R_scipy.from_euler('xyz', [rx, ry, rz], degrees=True)
        else:
            rotation = R_scipy.from_euler('xyz', [rx, ry, rz], degrees=False)
        matrix = np.eye(4)
        
        matrix[:3, :3] = rotation.as_matrix() 
        
        if milimeter:
            
            matrix[:3, 3] = [x/1000, y/1000, z/1000]
        else:
            matrix[:3, 3] = [x, y, z]
        return matrix


    def matrix_to_pose(self,matrix, degrees=True,milimeter=True):
        """
        å°‡ 4x4 çš„é½Šæ¬¡è½‰æ›çŸ©é™£é‚„åŸç‚ºä½ç½®å’Œæ­æ‹‰è§’ã€‚
        
        åƒæ•¸:
            matrix: 4x4 çš„é½Šæ¬¡è½‰æ›çŸ©é™£ï¼ˆnumpy.ndarrayï¼‰
            degrees: å¦‚æœç‚º Trueï¼Œå‰‡è¼¸å‡ºçš„è§’åº¦ç‚ºåº¦ï¼›å¦å‰‡ç‚ºå¼§åº¦ã€‚
        
        è¿”å›:
            (x, y, z, rx, ry, rz): ä½ç½®å’Œå¹³ç§»é‡
        """
        if degrees:
            rotation = R_scipy.from_matrix(matrix[:3, :3])
            rx, ry, rz = rotation.as_euler('xyz', degrees=True)
        else:
            rotation = R_scipy.from_matrix(matrix[:3, :3])
            rx, ry, rz = rotation.as_euler('xyz', degrees=False)

        if milimeter:
            x, y, z = matrix[:3, 3]/1000
        else:
            x,y,z=matrix[:3, 3]
        return x, y, z, rx, ry, rz


    '''def get_mask_data(self,image_src):
        image=image_src.copy()
        results = model.predict(source=image,verbose=False)
        
        # è®€å–åŸå§‹åœ–ç‰‡
        # original_image = cv2.imread(image)
        original_image = image
        mask_pixel_list=[]
        class_id_list=[]
        confidence_score_list=[]

        for result in results:
            masks = result.masks  # ç²å–åˆ†å‰²é®ç½©  
            classes = result.boxes.cls.cpu().numpy()  # ç²å–é¡åˆ¥ç´¢å¼•  
            confidence=result.boxes.conf.cpu().numpy()  # ç²å–ç½®ä¿¡åº¦
            if masks is not None:
                for mask, cls,conf in zip(masks.data, classes,confidence):
                    # å°‡é®ç½©è½‰æ›ç‚ºäºŒå€¼åœ–åƒ
                    mask = mask.cpu().numpy().astype(np.uint8) * 255
                    # print(conf)
                    confidence_score_list.append(conf)
                    
                    # ç²å–å°æ‡‰é¡åˆ¥çš„é¡è‰²
                    color = class_colors.get(int(cls), (255, 255, 255))  # é»˜èªç™½è‰²
                    class_name = class_names_dict.get(int(cls), "Unknown")
                    class_id_list.append(class_name)

                    # ç²å–é®ç½©å…§éƒ¨æ‰€æœ‰åƒç´ çš„åº§æ¨™
                    y_coords, x_coords = np.where(mask > 0)  # ç²å–éé›¶åƒç´ çš„ y å’Œ x åº§æ¨™
                    pixel_coords = np.array([x_coords, y_coords])  # 2Ã—N çŸ©é™£
                    mask_pixel_list.append(pixel_coords.T)

                    """ 
                    # æ¸¬è©¦é®ç½©åº§æ¨™æ˜¯å¦æ­£ç¢º
                    for x,y in pixel_coords.T:
                        original_image[y,x]=(0,0,0)
                    print(pixel_coords[:,0])
                    """

                    # exit()

                    # å°‡é®ç½©ç–ŠåŠ åˆ°åŸå§‹åœ–ç‰‡
                    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                    cv2.drawContours(original_image, contours, -1, color, thickness=1)
                    
                    # å‰µå»ºä¸€å€‹èˆ‡åŸå§‹åœ–ç‰‡ç›¸åŒå¤§å°çš„é€æ˜é®ç½©
                    overlay = original_image.copy()
                    alpha = 0.5  # è¨­å®šé€æ˜åº¦ (0.0 å®Œå…¨é€æ˜, 1.0 å®Œå…¨ä¸é€æ˜)

                    # å°‡é®ç½©æ‡‰ç”¨é¡è‰²
                    overlay[mask > 0] = color

                    # å°‡é€æ˜é®ç½©ç–ŠåŠ åˆ°åŸå§‹åœ–ç‰‡
                    cv2.addWeighted(overlay, alpha, original_image, 1 - alpha, 0, original_image)
                        # è¨ˆç®—é®ç½©çš„ä¸­å¿ƒé»
                    for contour in contours:
                        M = cv2.moments(contour)
                        if M["m00"] > 0:  # é˜²æ­¢é™¤ä»¥é›¶
                            cX = int(M["m10"] / M["m00"])
                            cY = int(M["m01"] / M["m00"])
                            # åœ¨ä¸­å¿ƒé»æ·»åŠ é¡åˆ¥æ–‡å­—æ¨™ç±¤
                            cv2.putText(original_image, class_name, (cX, cY), cv2.FONT_HERSHEY_SIMPLEX, 
                                        0.6, color, 2, cv2.LINE_AA)
                    print(class_name,conf,pixel_coords.T.shape)
        self.mark_image=original_image
        self.mask_pixel_list=mask_pixel_list
        self.class_id_list=class_id_list
        self.confidence_score_list=confidence_score_list
                    # break
        return original_image,mask_pixel_list,class_id_list,confidence_score_list'''


    def pixel_to_3d_points(self,mask_pixel_np, depth_image, intrinsics ):
        """
        å°‡ mask çš„ N x 2 pixel åº§æ¨™è½‰ç‚º 3D ç©ºé–“åº§æ¨™ï¼ˆN x 3ï¼‰

        mask_pixel_np: (N, 2)ï¼Œæ¯åˆ—æ˜¯ [x, y]
        depth_image: å°é½Š color çš„æ·±åº¦åœ–ï¼ˆå–®ä½ mmï¼‰
        intrinsics: RealSense ç›¸æ©Ÿå…§åƒï¼ˆå°é½Š color ç”¨ color stream çš„å…§åƒï¼‰
        """
        points_3d = []
        pose = self.ask_flange_pose()
        T_flange2base = self.pose_to_matrix(pose)
        T_cam2gripper=np.array(
        [[-0.995,0.099,-0.017,0.038],
        [-0.098,-0.992,-0.079,-0.075],
        [-0.025,-0.076,0.997,0.038],
        [0.000,0.000,0.000,1.000]])

        for x, y in mask_pixel_np:
            depth = depth_image[y, x] * 0.001  # mm â†’ m
            if depth == 0: continue  # å¿½ç•¥ç„¡æ•ˆé»
            pt3d = rs.rs2_deproject_pixel_to_point(intrinsics, [x, y], depth)  # å›å‚³ [X, Y, Z]
            points_3d.append(pt3d)
        
        
        # å°‡ points_3d è½‰æ›ç‚º 4 x M çš„é½Šæ¬¡çŸ©é™£å½¢å¼
        points_3d = np.array(points_3d)  # åŸå§‹ M x 3
        points_3d_homogeneous = np.hstack((points_3d, np.ones((points_3d.shape[0], 1)))).T  # è½‰æ›ç‚º 4 x M
        points_3d_in_base= (T_flange2base @ T_cam2gripper @ points_3d_homogeneous).T[:, :3]  # è½‰æ›ç‚º base åº§æ¨™ç³»
        return points_3d_in_base # shape (M, 3)


    def merge_mask_points(self):
        """
        å°‡ mask çš„ N x 2 pixel åº§æ¨™è½‰ç‚º 3D ç©ºé–“åº§æ¨™ï¼ˆN x 3ï¼‰

        mask_pixel_np: (N, 2)ï¼Œæ¯åˆ—æ˜¯ [x, y]
        depth_image: å°é½Š color çš„æ·±åº¦åœ–ï¼ˆå–®ä½ mmï¼‰
        intrinsics: RealSense ç›¸æ©Ÿå…§åƒï¼ˆå°é½Š color ç”¨ color stream çš„å…§åƒï¼‰
        """
        all_instance_points = []
        all_instance_original_colors = []  # æ–°å¢ä¸€å€‹åˆ—è¡¨ä¾†å­˜å„²æ¯å€‹å¯¦ä¾‹çš„é¡è‰²
        all_instance_draw_colors = []  # æ–°å¢ä¸€å€‹åˆ—è¡¨ä¾†è‘—è‰²

        for i, pix2d in enumerate(self.mask_pixel_list):
            valid_pix2d = pix2d[self.depth_image[pix2d[:, 1], pix2d[:, 0]] > 0]  # éæ¿¾æ‰æ·±åº¦å€¼ç‚º 0 çš„åƒç´ 
            # print(valid_pix2d.shape)
            pts3d = self.pixel_to_3d_points(valid_pix2d, self.depth_image, self.intrinsics)
            all_instance_points.append(pts3d)


            # æ ¹æ“šé¡åˆ¥åç¨±æŸ¥æ‰¾é¡è‰²
            class_name = self.class_id_list[i]
            class_index = next((k for k, v in class_names_dict.items() if v == class_name), None)
            color = class_colors.get(class_index, (255, 255, 255))  # é»˜èªç™½è‰²
            normalized_color = [c / 255.0 for c in color]
            all_instance_draw_colors.extend([normalized_color] * len(pts3d))

            # æå–æœ‰æ•ˆåƒç´ çš„åŸå§‹é¡è‰²
            original_colors = color_image[valid_pix2d[:, 1], valid_pix2d[:, 0]]
            normalized_colors = original_colors / 255.0
            all_instance_original_colors.extend(normalized_colors)
            print(np.array(all_instance_draw_colors).shape,np.array(all_instance_original_colors).shape)
        self.all_instance_points = all_instance_points
        self.all_instance_original_colors = all_instance_original_colors
        self.all_instance_draw_colors = all_instance_draw_colors
        return all_instance_points, all_instance_original_colors, all_instance_draw_colors
    
    def generate_o3d_pointcloud(self):
        np_all_instance_points = np.vstack(self.all_instance_points)
        np_all_instance_original_colors = np.array(self.all_instance_original_colors)[:,::-1]
        np_all_instance_draw_colors = np.array(self.all_instance_draw_colors)[:,:]
        pcd=o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(np_all_instance_points)
        pcd.colors = o3d.utility.Vector3dVector(np_all_instance_draw_colors)  # è¨­å®šé»é›²é¡è‰²
        # pcd.colors = o3d.utility.Vector3dVector(np_all_instance_original_colors)
        return pcd      

    def get_mask_data_accurate(self,image_src):
        def roi_cut(color_image, pt1:tuple=(0,0),pt2:tuple=(1280,720)):
            x1, y1 = pt1
            x2, y2 = pt2

            # === åˆ‡å‰² ROI ===
            color_image = color_image[y1:y2, x1:x2]
            
            return color_image,pt1

        image=image_src.copy()
        image,pt1=roi_cut(image,pt1=(230,10),pt2=(1040,700))
        
        results = model.predict(
            source=image,
            verbose=False,
            save=False,
            retina_masks=True,  # â† âœ… ç¢ºä¿è¼¸å‡ºé«˜è§£æåº¦é®ç½©
            device='cpu'
        )
        x1,y1=pt1
        # image = cv2.imread(image_path)
        h, w = image.shape[:2]

        mask_pixel_list = []
        class_id_list = []
        confidence_score_list = []

        for result in results:
            masks = result.masks.data  # [n, h, w] torch.Tensor
            classes = result.boxes.cls.cpu().numpy()
            scores = result.boxes.conf.cpu().numpy()

            for i, (cls, conf) in enumerate(zip(classes, scores)):
                class_name = class_names_dict.get(int(cls), "Unknown")
                class_id_list.append(class_name)
                confidence_score_list.append(conf)

                # æå–å–®ä¸€ mask
                mask_tensor = masks[i]  # shape: (h, w)
                mask = mask_tensor.cpu().numpy().astype(np.uint8) * 255
                mask_resized = cv2.resize(mask, (w, h), interpolation=cv2.INTER_NEAREST)

                # ç²å–é®ç½©åƒç´ é»ä½ç½®
                y, x = np.where(mask_resized > 0)
                y=y+y1
                x=x+x1
                pixel_coords = np.stack([x, y], axis=1)
                mask_pixel_list.append(pixel_coords)

                # é®ç½©ä¸Šè‰²èˆ‡ç–ŠåŠ 
                color = class_colors.get(int(cls), (255, 255, 255))
                overlay = image.copy()
                contours, _ = cv2.findContours(mask_resized, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                cv2.drawContours(overlay, contours, -1, color, thickness=cv2.FILLED)
                image = cv2.addWeighted(overlay, 0.5, image, 0.5, 0)

                # é‚Šæ¡†èˆ‡æ–‡å­—
                cv2.drawContours(image, contours, -1, color, thickness=2)
                for contour in contours:
                    M = cv2.moments(contour)
                    if M["m00"] > 0:
                        cX = int(M["m10"] / M["m00"])
                        cY = int(M["m01"] / M["m00"])
                        label = f"{class_name} {conf:.2f}"
                        cv2.putText(image, label, (cX, cY), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

        return image, mask_pixel_list, class_id_list, confidence_score_list 
        
def preprocess_point_cloud(pcd, voxel_size=0.003)->o3d.geometry.PointCloud:
    pcd_down = pcd.voxel_down_sample(voxel_size)
    pcd_down.estimate_normals(
        search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2, max_nn=30)
    )
    return pcd_down

def icp_align_origin(source, target, threshold=0.02):
    result = o3d.pipelines.registration.registration_icp(
        source, target, threshold, np.eye(4),
        o3d.pipelines.registration.TransformationEstimationPointToPoint()
    )
    return result.transformation

def icp_align_robust_plane(source, target, threshold=0.01, sigma=0.01):
    loss = o3d.pipelines.registration.TukeyLoss(k=sigma)
    estimator = o3d.pipelines.registration.TransformationEstimationPointToPlane(loss)
    result = o3d.pipelines.registration.registration_icp(
        source, target, threshold, np.identity(4), estimator
    )
    return result.transformation, result

def o3d_to_ros_pointcloud(pcd, frame_id="base"):
    # å°‡ Open3D çš„é»é›²è½‰ç‚º PointCloud2 ROS è¨Šæ¯
    points = np.asarray(pcd.points)
    colors = (np.asarray(pcd.colors) * 255).astype(np.uint8)
    data = []

    for i in range(len(points)):
        x, y, z = points[i]
        r, g, b = colors[i]
        rgb = int(r) << 16 | int(g) << 8 | int(b)
        data.append([x, y, z, rgb])

    header = Header()
    header.stamp = rospy.Time.now()
    header.frame_id = frame_id

    fields = [
        PointField('x', 0, PointField.FLOAT32, 1),
        PointField('y', 4, PointField.FLOAT32, 1),
        PointField('z', 8, PointField.FLOAT32, 1),
        PointField('rgb', 12, PointField.UINT32, 1),
    ]

    pc2_msg = pcl2.create_cloud(header, fields, data)
    return pc2_msg

def post_process(depth_frame):
    # === Disparity transformation ===
    depth_to_disparity = rs.disparity_transform(True)
    disparity_to_depth = rs.disparity_transform(False)
    depth_frame = depth_to_disparity.process(depth_frame)

    # === Decimation Filter ===
    decimation = rs.decimation_filter()
    decimation.set_option(rs.option.filter_magnitude, 1)
    depth_frame = decimation.process(depth_frame)

    # === Spatial Filter ===
    spatial = rs.spatial_filter(
        smooth_alpha=0.25, 
        smooth_delta=50.0, 
        magnitude=3.0,
        hole_fill=2.0
    )
    depth_frame = spatial.process(depth_frame)

    # === Temporal Filter ===
    temporal = rs.temporal_filter(0.25, 100.0, 8)  # alpha, delta, persistency_mode
    depth_frame = temporal.process(depth_frame)

    # === Convert back to depth ===
    depth_frame = disparity_to_depth.process(depth_frame)

    # === Hole Filling ===
    # hole_filling = rs.hole_filling_filter()
    # hole_filling.set_option(rs.option.holes_fill, 1)
    # depth_frame = hole_filling.process(depth_frame)

    return depth_frame


# å®šç¾©è½‰ç§»çŸ©é™£çš„å‡½æ•¸
def R_and_t_to_T(R, t):
    T = np.hstack((R, t.reshape(-1, 1)))  # å°‡å¹³ç§»å‘é‡è½‰æ›ç‚ºåˆ—å‘é‡
    T = np.vstack((T, [0, 0, 0, 1]))  # æ·»åŠ æœ€å¾Œä¸€è¡Œ
    return T

def T_to_R_and_t(T):
    Rt = T[:3]
    R = Rt[:, :3]
    t = Rt[:, 3].reshape((-1, 1))
    return R, t
  
def collision_detect_z(position, approach, binormal, axis, bbox_id,
                                   hand_depth=0.07, hand_height=0.02,
                                   outer_diameter=0.105, finger_width=0.01,
                                   table_z=0.08828, frame_id="base"):
    hw = 0.5 * outer_diameter - 0.5 * finger_width
    R = np.column_stack((approach, binormal, axis))
    z_compensation = 0.0
    z_compensation_mm=0.0
    left_center = position - hw * binormal + 0.5 * hand_depth * approach
    right_center = position + hw * binormal + 0.5 * hand_depth * approach
    left_tip = position - hw * binormal + hand_depth * approach
    right_tip = position + hw * binormal + hand_depth * approach

    # markers = MarkerArray()

    # # left & right box
    # for idx, center in enumerate([left_center, right_center]):
    #     marker = Marker()
    #     marker.header.frame_id = frame_id
    #     marker.header.stamp = rospy.Time.now()
    #     marker.ns = "bbox"
    #     marker.id = bbox_id * 10 + idx
    #     marker.type = Marker.CUBE
    #     marker.action = Marker.ADD
    #     marker.pose.position.x, marker.pose.position.y, marker.pose.position.z = center
    #     quat = R_scipy.from_matrix(R).as_quat()
    #     marker.pose.orientation.x, marker.pose.orientation.y, marker.pose.orientation.z, marker.pose.orientation.w = quat
    #     marker.scale.x, marker.scale.y, marker.scale.z = hand_depth, finger_width, hand_height
    #     marker.color.r, marker.color.g, marker.color.b, marker.color.a = 0.0, 1.0, 0.0, 0.4
    #     marker.lifetime = rospy.Duration(30)
    #     markers.markers.append(marker)

    # fingertips
    # for idx, tip in enumerate([left_tip, right_tip]):
    #     marker = Marker()
    #     marker.header.frame_id = frame_id
    #     marker.header.stamp = rospy.Time.now()
    #     marker.ns = "fingertip"
    #     marker.id = bbox_id * 10 + 2 + idx
    #     marker.type = Marker.SPHERE
    #     marker.action = Marker.ADD
    #     marker.pose.position.x, marker.pose.position.y, marker.pose.position.z = tip
    #     marker.scale.x = marker.scale.y = marker.scale.z = 0.01
    #     marker.color.r, marker.color.g, marker.color.b, marker.color.a = 1.0, 0.0, 0.0, 1.0
    #     marker.lifetime = rospy.Duration(30)
    #     markers.markers.append(marker)

    # ç¢°æ’æª¢æŸ¥
    tip_z_min = min(left_tip[2], right_tip[2])
    is_danger = tip_z_min < table_z
    if is_danger:
            z_compensation = table_z+0.005- tip_z_min
            z_compensation_mm = z_compensation * 1000  # è½‰æ›ç‚º mm
            rospy.logwarn(f"âš ï¸ æŒ‡å°–ç¢°æ’æª¢æ¸¬ï¼štip_z_min={tip_z_min} , éœ€è¦è£œå„Ÿ {z_compensation_mm:.3f} mm")
            
    
    return z_compensation_mm, is_danger


def grasp_callback(msg,combined_pcd):
    # global T_grasps_in_base_list,T_grasps_in_base_list,T_tool_list  # ä½¿ç”¨å…¨åŸŸè®Šæ•¸
    
    
    T_grasps_in_base_list = []
    T_tool_list=[]
    gripper_base_position_mm_in_base_list= []
    tcp_pos_mm_in_base_list = []
    class_in_base_list = []
    
    rospy.loginfo("æ”¶åˆ° %d å€‹æŠ“å–å§¿æ…‹", len(msg.grasps))
    if len(msg.grasps) !=0:
        have_grasps=True
        for i, grasp in enumerate(msg.grasps):
            # å°‡æŠ“å–å§¿æ…‹çš„è³‡è¨Šè½‰æ›ç‚º np.array
            gripper_base_position = np.array([grasp.position.x, grasp.position.y, grasp.position.z])
            approach = np.array([grasp.approach.x, grasp.approach.y, grasp.approach.z])
            binormal = np.array([grasp.binormal.x, grasp.binormal.y, grasp.binormal.z])
            axis = np.array([grasp.axis.x, grasp.axis.y, grasp.axis.z])
            sample = np.array([grasp.sample.x, grasp.sample.y, grasp.sample.z]) 
            
            # === æ¨ç®— TCP ä¸–ç•Œåº§æ¨™ ===
            hand_depth = 0.07        # 7cm å¤¾çˆªæ·±åº¦
            deepen_offset = 0.005     # è‹¥é–‹å•Ÿ deepen_handï¼Œé€™è£¡æ‡‰è¨­ç‚º 0.01
            weight=1 # å¤¾çˆªæ·±åº¦çš„æ¬Šé‡ï¼Œè‹¥æœ‰ TCP_frame_transformï¼Œå‰‡è¨­ç‚º 1
            tcp_offset = hand_depth / weight - deepen_offset
            tcp_position = gripper_base_position + tcp_offset * approach
            tcp_position_long = gripper_base_position + (hand_depth ) * approach  # é•·åº¦æ–¹å‘çš„ TCP ä½ç½®
            
            # è¨ˆç®—åŒ…åœé•·æ–¹é«”çš„ç¯„åœ
            box_length = np.linalg.norm(tcp_position_long - gripper_base_position+0.01)  # é•·æ–¹é«”çš„é•·åº¦
            box_width = 20 / 1000  # é•·æ–¹é«”çš„å¯¬åº¦ï¼ˆ0.02mmï¼‰
            box_height = 20 / 1000  # é•·æ–¹é«”çš„é«˜åº¦ï¼ˆ0.02mmï¼‰
            # éæ­·é»é›²ï¼Œç¯©é¸å‡ºåœ¨åŒ…åœé•·æ–¹é«”å…§çš„é»

            points_in_box = []
            colors_in_box = []
            for point, color in zip(combined_pcd.points, combined_pcd.colors):
                # è¨ˆç®—é»åˆ° position çš„å‘é‡
                vector_to_point = np.array(point) - gripper_base_position

                # è¨ˆç®—åœ¨ approach, binormal, axis ä¸Šçš„æŠ•å½±
                proj_length = np.dot(vector_to_point, approach)
                proj_width = np.dot(vector_to_point, binormal)
                proj_height = np.dot(vector_to_point, axis)

                # åˆ¤æ–·é»æ˜¯å¦åœ¨åŒ…åœé•·æ–¹é«”å…§
                if (0 <= proj_length <= box_length and
                    -box_width / 2 <= proj_width <= box_width / 2 and
                    -box_height / 2 <= proj_height <= box_height / 2):
                    points_in_box.append(point)
                    colors_in_box.append(color)

            # çµ±è¨ˆç¯„åœå…§çš„ä¸»è¦é¡è‰²
            if colors_in_box:
                colors_in_box = np.array(colors_in_box)
                avg_color = np.mean(colors_in_box, axis=0)  # è¨ˆç®—å¹³å‡é¡è‰²
                # print(f"æŠ“å–å§¿æ…‹ {i} çš„ä¸»è¦é¡è‰²: {avg_color}")

                # åˆ¤æ–·ç‰©é«”é¡å‹
                for class_id, class_color in class_colors.items():
                    normalized_color = np.array(class_color) / 255.0
                    if np.allclose(avg_color, normalized_color, atol=0.1):  # é¡è‰²ç›¸ä¼¼åˆ¤æ–·
                        # print(f"æŠ“å–å§¿æ…‹ {i} çš„ç‰©é«”é¡å‹: {class_names_dict[class_id]}")
                        class_in_base_list.append(class_names_dict[class_id])
                        break
                class_in_base_list.append(None)    
                
            else:
                print(f"æŠ“å–å§¿æ…‹ {i} çš„ç¯„åœå…§æ²’æœ‰é»é›²")
                class_in_base_list.append(None)

            # === å»ºç«‹ grasp frame ===
            R_grasp = np.column_stack((approach, binormal, axis))

            # å»ºç«‹è½‰ç§»çŸ©é™£
            T_tcp = R_and_t_to_T(R_grasp, tcp_position) 
            
            T_grasp_in_base = T_tcp  # é€™å°±æ˜¯æŠ“å–å§¿æ…‹çš„ TCP

            T_grasps_in_base_list.append(T_grasp_in_base)

            # è‹¥è¦åˆ†é›¢å‡ºæ—‹è½‰èˆ‡å¹³ç§»éƒ¨åˆ†ï¼ˆå¯é¸ï¼‰
            # R_grasp_in_base,t_grasp_in_base = T_to_R_and_t(T_grasp_in_base)


            # === å°‡ TCP å†è½‰ç‚ºå·¥å…·åº§æ¨™ç³»ï¼ˆè‹¥æœ‰ TCP_frame_transformï¼‰ ===
            TCP_frame_transform = np.array([
                [0, 1, 0, 0],
                [0, 0, 1, 0],
                [1, 0, 0, 0],
                [0, 0, 0, 1]
            ])
            TCP_frame_transform_inv = np.linalg.inv(TCP_frame_transform)
            T_tool = T_grasp_in_base @ TCP_frame_transform_inv
            T_tool_list.append(T_tool)

            # åˆ†è§£å§¿æ…‹
            R_tool, t_tool = T_to_R_and_t(T_tool)
            euler_angles = R_scipy.from_matrix(R_tool).as_euler('xyz', degrees=True)
            rx, ry, rz = euler_angles

            # Z è»¸æ–¹å‘ç¿»è½‰è£œæ­£ï¼ˆè‹¥ rz ç‚ºè² ï¼‰
            if rz < 0:
                R_flip = R_scipy.from_euler('z', 180, degrees=True).as_matrix()
                T_flip = np.eye(4)
                T_flip[:3, :3] = R_flip
                T_tool = T_tool @ T_flip
                R_tool, t_tool = T_to_R_and_t(T_tool)
                rx, ry, rz = R_scipy.from_matrix(R_tool).as_euler('xyz', degrees=True)

            # TCP é»èˆ‡ sample é»ä»¥ mm ç‚ºå–®ä½è¼¸å‡º
            tcp_pos_mm = t_tool * 1000
            gripper_base_position_mm = gripper_base_position * 1000
            x0, y0, z0 = gripper_base_position_mm
            x, y, z = [float(v) for v in tcp_pos_mm]
            
            z_compensation_mm, is_danger= collision_detect_z(
                gripper_base_position, approach, binormal, axis, bbox_id=i,
                hand_depth=0.07, hand_height=0.02,
                outer_diameter=0.105, finger_width=0.01,
                table_z=0.08828, frame_id="base"
            )
        
            if is_danger:
                # rospy.logwarn(f"[Grasp {i}] âš ï¸ æŒ‡å°–å¯èƒ½ç¢°æ’æ¡Œé¢ï¼\n"
                #               f"origin_gripper_base_position x={x0:.2f}, y={y0:.2f}, z={z0:.2f}, rx={rx:.2f}, ry={ry:.2f}, rz={rz:.2f}\n"
                #               f"origin_TCP x={x:.2f}, y={y:.2f}, z={z:.2f}, rx={rx:.2f}, ry={ry:.2f}, rz={rz:.2f}\n"
                #               )
                z0 += z_compensation_mm
                z  += z_compensation_mm
            # å„²å­˜çµæœ
            gripper_base_position_mm_in_base_list.append([x0, y0, z0, rx, ry, rz])
            tcp_pos_mm_in_base_list.append([x, y, z, rx, ry, rz])
            # è¼¸å‡ºé¡¯ç¤º
        #     color_print(f'[gripper_base_position ] x={x0:.2f}, y={y0:.2f}, z={z0:.2f}, rx={rx:.2f}, ry={ry:.2f}, rz={rz:.2f}', color='cyan')
        #     color_print(f'[TCP] x={x:.2f}, y={y:.2f}, z={z:.2f}, rx={rx:.2f}, ry={ry:.2f}, rz={rz:.2f}', color='cyan')

        color_print(f'Best_gripper_base_position = {gripper_base_position_mm_in_base_list[0]}', color='green')
        color_print(f'Best_TCP_position = {tcp_pos_mm_in_base_list[0]}', color='green')
        print('len =', len(T_grasps_in_base_list))

    else:
        rospy.logwarn("æ²’æœ‰æ”¶åˆ°ä»»ä½•æŠ“å–å§¿æ…‹ï¼Œè«‹æª¢æŸ¥è¼¸å…¥è¨Šæ¯")
        have_grasps = False
        return [], [], [],have_grasps 
    return gripper_base_position_mm_in_base_list, tcp_pos_mm_in_base_list, class_in_base_list,have_grasps


def color_str(text, color="white"):
    color_codes = {
        "black":   "\033[30m",
        "red":     "\033[31m",
        "green":   "\033[32m",
        "yellow":  "\033[33m",
        "blue":    "\033[34m",
        "magenta": "\033[35m",
        "cyan":    "\033[36m",
        "white":   "\033[37m"
    }
    reset_code = "\033[0m"

    color = color.lower()
    if color not in color_codes:
        print(f"\033[31m[éŒ¯èª¤] ä¸æ”¯æ´çš„é¡è‰²ï¼š{color}\033[0m")
        return f"{color_codes[color]}{text}{reset_code}"

    

def color_print(text, color="white"):
    color_codes = {
        "black":   "\033[30m",
        "red":     "\033[31m",
        "green":   "\033[32m",
        "yellow":  "\033[33m",
        "blue":    "\033[34m",
        "magenta": "\033[35m",
        "cyan":    "\033[36m",
        "white":   "\033[37m"
    }
    reset_code = "\033[0m"

    color = color.lower()
    if color not in color_codes:
        print(f"\033[31m[éŒ¯èª¤] ä¸æ”¯æ´çš„é¡è‰²ï¼š{color}\033[0m")
        return

    print(f"{color_codes[color]}{text}{reset_code}")




# éæ­·è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰åœ–ç‰‡
def get_image_path_list(test_image_path = r"/home/chen/Segmentation_Train/train_data/test/images"):
    # ç¢ºä¿æ¸¬è©¦åœ–ç‰‡å­˜åœ¨
    if not os.path.exists(test_image_path):
        raise FileNotFoundError(f"æ¸¬è©¦åœ–ç‰‡ä¸å­˜åœ¨: {test_image_path}")
    img_list=[]
    for image_name in os.listdir(test_image_path):
        # ç¢ºä¿æ˜¯åœ–ç‰‡æª”æ¡ˆ
        if image_name.lower().endswith(('.jpg', '.jpeg', '.png')):
            image_path = os.path.join(test_image_path, image_name)
            img_list.append(image_path)
            # print(f"æ­£åœ¨è™•ç†åœ–ç‰‡: {image_path}")
    return img_list

if __name__ == "__main__":
    rospy.init_node('mask_point_cloud', anonymous=True)
    cloud_publisher=rospy.Publisher('cloud_stitched', PointCloud2, queue_size=10)

    gripper_pick(pick_distance=0,inital_bool=False)
    change_tcp("ChangeTCP(\"robotiq_origin_gripper\")")
    

    VIEW_POSE_1=[464.74 , -199.48 , 381.93 , 146.32 , 4.39 , 165.82] #TCP
    VIEW_POSE_2=[465.60 , 56.86 , 409.25 , 145.48 , 2.77 , 1.23] #TCP
    VIEW_POSE_3=[355.23 , -88.64 , 364.20 , 144.28 , -0.96 , 94.84] #TCP
    VIEW_POSE_LIST=[VIEW_POSE_1,VIEW_POSE_2,VIEW_POSE_3]
    PREPARE_POSE=[552.75 , -299.90 , 332.59 , 146.32 , 4.39 , 90.84] #TCP
    PUT_POSE_0=[655.84 , 499.26 , 239.60 , -180.00 , 0.00 , 180] #TCP
    PUT_POSE_1=[459.60 , 499.26 , 239.60 , -180.00 , 0.00 , 180] #TCP
    PUT_POSE_2=[252.89 , 499.26 , 239.60 , -180.00 , 0.00 , 180] #TCP
    RECYCLE_POSE=[-35.03 , 776.64 , 233.22, 180.00 ,0.00 ,-180.00]#TCP
    
    PUT_DOWN_POSE_0=[655.84 , 499.26 , 207.27 , -180.00 , 0.00 , 180] #TCP
    PUT_DOWN_POSE_1=[459.60 , 499.26 , 207.27 , -180.00 , 0.00 , 180] #TCP
    PUT_DOWN_POSE_2=[252.89 , 499.26 , 207.27 , -180.00 , 0.00 , 180] #TCP
    PUT_DOWN_RECYCLE_POSE=[-35.03 , 776.64 , 183.32, 180.00 ,0.00 ,-180.00]

    # Create a pipeline
    pipeline = rs.pipeline()
    # Create a config and configure the pipeline to stream
    #  different resolutions of color and depth streams
    CONFIG = rs.config()
    CONFIG.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 30)
    CONFIG.enable_stream(rs.stream.color, 1280, 720, rs.format.bgr8, 30)
    # Start streaming
    profile = pipeline.start(CONFIG)
    device = profile.get_device()

    # === Apply JSON Advanced Settings ===
    try:
        advanced_mode = rs.rs400_advanced_mode(device)
        if not advanced_mode.is_enabled():
            print("ğŸ›  æ­£åœ¨å•Ÿç”¨ advanced mode ...")
            advanced_mode.toggle_advanced_mode(True)
            import time
            time.sleep(1)
        # å¥—ç”¨ JSON
        with open('/home/chen/realsense_setting/realsesnse_setting3.json', 'r') as file:
            json_text = file.read()
            advanced_mode.load_json(json_text)
        print("âœ… JSON è¨­å®šå·²æˆåŠŸè¼‰å…¥")
    except Exception as e:
        print("âš ï¸ è£ç½®ä¸æ”¯æ´ advanced_modeï¼Œæˆ–åˆå§‹åŒ–å¤±æ•—ï¼š", e)


    align_to = rs.stream.color
    align = rs.align(align_to)

    while not rospy.is_shutdown():
    # Streaming loop
        view_point_cloud_list=[]
        move_script_with_monitor(VIEW_POSE_3,motion_type='PTP')
        for i, view_pose in enumerate(VIEW_POSE_LIST):
            # input(f"\nğŸ“¸ ç§»å‹•åˆ°ç¬¬ {i+1} è¦–è§’å¾ŒæŒ‰ Enter æ“·å–...")
            move_script_with_monitor(view_pose,motion_type='PTP')
            
            for _ in range(10): frames = pipeline.wait_for_frames()  # ç­‰å¹¾å¹€è®“è³‡æ–™ç©©å®š
            # Align the depth frame to color frame
            aligned_frames = align.process(frames)

            # Get aligned frames
            aligned_depth_frame = aligned_frames.get_depth_frame()  # aligned_depth_frame is a 640x480 depth image
            color_frame = aligned_frames.get_color_frame()

            # æ‡‰ç”¨ Hole Filling Filter
            filtered_depth_frame = post_process(aligned_depth_frame)

            # å°‡æ·±åº¦å½±åƒè½‰æ›ç‚º NumPy é™£åˆ—
            depth_image = np.asanyarray(filtered_depth_frame.get_data())
            color_image = np.asanyarray(color_frame.get_data())
            
            cv2.imwrite(os.path.join(image_path_test, f'{i}_detect.png'), color_image)
            # exit()
            view_point_cloud_list.append(
                view_point_cloud(i,
                                color_image,
                                depth_image,
                                profile.get_stream(rs.stream.color).as_video_stream_profile().get_intrinsics()))
            
                    # é è™•ç†é»é›²
        # processed_point_clouds = [preprocess_point_cloud(vpc.o3d_pcd) for vpc in view_point_cloud_list]
        processed_point_clouds = [vpc.o3d_pcd for vpc in view_point_cloud_list]
        
        # è¨­å®šåŸºæº–é»é›²
        target = processed_point_clouds[0]
        combined_pcd = target  # åˆå§‹åŒ–åˆä½µé»é›²
        
        # ç–Šåˆå…¶ä»–é»é›²åˆ°åŸºæº–é»é›²
        for i in range(1, len(processed_point_clouds)):
            source = processed_point_clouds[i]
            print(f"æ­£åœ¨é€²è¡Œç¬¬ {i} å€‹é»é›²çš„ ICP ç–Šåˆ...")
            
            # # ä½¿ç”¨ icp_align_robust_plane é€²è¡Œç–Šåˆ
            # transformation, result = icp_align_robust_plane(source, target, threshold=0.02, sigma=0.05)
            # print(f"ICP ç–Šåˆå®Œæˆï¼Œè½‰æ›çŸ©é™£ï¼š\n{transformation}")
            
            # # å°‡è½‰æ›çŸ©é™£æ‡‰ç”¨åˆ°é»é›²
            # source.transform(transformation)
            
            # åˆä½µé»é›²
            combined_pcd += source
        
        points = np.asarray(combined_pcd.points)
        # è¨­å®š Z å€¼ä¸‹é™
        z_threshold = 0.07
        # å»ºç«‹å¸ƒæ—é®ç½©ï¼Œåªä¿ç•™ Z å€¼ >= z_threshold çš„é»
        mask = points[:, 2] >= z_threshold
        # å¥—ç”¨é®ç½©ä¸¦å»ºç«‹æ–°çš„é»é›²ç‰©ä»¶
        filtered_pcd = combined_pcd.select_by_index(np.where(mask)[0])
        
        # combined_pcd = filtered_pcd   
        # cl, ind = combined_pcd.remove_statistical_outlier(nb_neighbors=50, std_ratio=2.0)
        
        # combined_pcd=combined_pcd.select_by_index(ind)  
        combined_pcd=filtered_pcd
            
                
        # å»ºç«‹åº§æ¨™ç³»
        axis = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1, origin=[0, 0, 0])
        
        
        # è¨­å®šè¦–è§’åƒæ•¸
        zoom = 2
        
        front = [-0.5, 0.0, 0.0]
        lookat = [0.0, 0.0, 0.0]
        up = [0.0, 0.0, 1.0]

        # msg=o3d_to_ros_pointcloud(pcd_combined)
        o3d.io.write_point_cloud("/home/chen/catkin_ws/src/pcl_with_gpd/scripts/test.ply", combined_pcd, write_ascii=True)
        cloud_publisher.publish(o3d_to_ros_pointcloud(combined_pcd))
        # é¡¯ç¤ºé»é›²å’Œåº§æ¨™ç³»
        geometries_to_draw = [combined_pcd] + [axis]
        
        '''o3d.visualization.draw_geometries(geometries_to_draw,
                                        zoom=zoom,
                                        front=front,
                                        lookat=lookat,
                                        up=up,
                                        window_name="RGB PointCloud from RealSense") 
                # Display the color image with ROI
            # cv2.imshow('Color Image with ROI', color_image)

            # # Display the point cloud
            # if input("go")=="go":
            #     o3d.visualization.draw_geometries([pcd])

            # key = cv2.waitKey(0)
            # if key & 0xFF == ord('q'):
            #     break
            # elif key & 0xff== ord('m'):
            #     o3d.visualization.draw_geometries([pcd])
            #     continue
                                        '''
        voxelize_pcd=combined_pcd.voxel_down_sample(voxel_size=0.005)  # å°‡é»é›²é€²è¡Œé«”ç´ åŒ–è™•ç†
        # bbox_array_pub = rospy.Publisher("/grasp_bbox_array", MarkerArray, queue_size=1)
            
        print("ç­‰å¾…å§¿æ…‹æ¶ˆæ¯...")
        have_grasps=False
        
        while  not rospy.is_shutdown():
            # color_print("æ²’æœ‰æŠ“å–å§¿æ…‹ï¼Œè«‹æª¢æŸ¥è¼¸å…¥è¨Šæ¯", "red")
            # continue
            msg = rospy.wait_for_message("/detect_grasps/clustered_grasps", GraspConfigList)  # ç­‰å¾…æ¥æ”¶æŠ“å–å§¿æ…‹æ¶ˆæ¯

            position_in_base_list,sample_in_base_list,class_in_base_list,have_grasps=grasp_callback(msg,voxelize_pcd)  # è™•ç†æ¥æ”¶åˆ°çš„æ¶ˆæ¯ 
            if input("æ˜¯å¦é‡ä¾†ï¼Ÿ(y/n): ").strip().lower() == 'y':
                cloud_publisher.publish(o3d_to_ros_pointcloud(combined_pcd))
                color_print("ç™¼é€é»é›²", "blue")
            else:
                break
            # ros_cloud=o3d_to_ros_pointcloud(pcd = o3d.io.read_point_cloud("/home/chen/catkin_ws/src/pcl_with_gpd/scripts/test.ply") , frame_id="base")
            
        
        
        for position ,sample,classes in zip(position_in_base_list,sample_in_base_list,class_in_base_list):
            if classes is not None:
                
                best_position= position
                best_sample= sample
                best_class=classes 
                break
            else: color_print("ç„¡æ³•åˆ†é¡ï¼Œå°‹æ‰¾ä¸‹ä¸€å€‹å§¿æ…‹ï¼", "yellow") 

        # exit()

        # move_script_with_monitor(PREPARE_POSE,motion_type='PTP')
        input(f'NEXT.................')
        move_script_with_monitor(best_position,motion_type='PTP')
        
        move_script_with_monitor(best_sample)
        gripper_pick(pick_distance=255,inital_bool=True)
        rospy.sleep(2)  # ç­‰å¾…å¤¾çˆªé–‰åˆ
        move_script_with_monitor(best_position,motion_type='PTP')
        # up_pose=best_position[:2]+[200]+best_position[3:]
        # move_script_with_monitor(up_pose)
        move_script_with_monitor(position[:2]+[385]+best_position[3:])


        obj_detect=get_gripper_position()
        if obj_detect !=2:
            
            color_print(f"obj_detect={obj_detect},ğŸ›  æª¢æ¸¬åˆ°å¤¾çˆªæœªå®Œå…¨é–‰åˆï¼Œæ­£åœ¨é–‰åˆå¤¾çˆª...", "red")
            gripper_pick(pick_distance=0,inital_bool=True)
            continue
        else:
            color_print("âœ… å¤¾çˆªå·²ç¶“é–‰åˆ", "green")
            

        if best_class == "aluextru":
            color_print(best_class, "red")
            move_script_with_monitor(PUT_POSE_0)
            move_script_with_monitor(PUT_DOWN_POSE_0)  
            gripper_pick(pick_distance=0,inital_bool=True)
            move_script_with_monitor(PUT_POSE_0)
    
        elif best_class == "bin":
            color_print(best_class, "green")
            move_script_with_monitor(PUT_POSE_1)
            # move_script_with_monitor(PUT_POSE_1[:2]+[161.9]+PUT_POSE_1[3:])
            move_script_with_monitor(PUT_DOWN_POSE_1)
            gripper_pick(pick_distance=0,inital_bool=True)
            move_script_with_monitor(PUT_POSE_1)
            
        elif best_class == "twpipe":
            color_print(best_class, "blue")
            move_script_with_monitor(PUT_POSE_2)
            # move_script_with_monitor(PUT_POSE_2[:2]+[161.9]+PUT_POSE_2[3:])
            move_script_with_monitor(PUT_DOWN_POSE_2)
            gripper_pick(pick_distance=0,inital_bool=True)
            move_script_with_monitor(PUT_POSE_2)
        else:
            color_print(best_class, "yellow")

        print("ä¸‹ä¸€è¼ªå¤¾å–...")
        input('next round .................')
        
        # cv2.destroyAllWindows()
    pipeline.stop()

